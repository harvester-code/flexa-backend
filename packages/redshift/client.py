import asyncio
import time

import redshift_connector
from fastapi import HTTPException
from loguru import logger
from sqlalchemy.pool import QueuePool

from packages.doppler.client import get_secret

# ============================================================
# NOTE: Redshift ì—°ê²°ì„ ìœ„í•œ ì„¤ì • (Optimized for AWS Redshift idle timeouts)

# ë™ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥í•œ POOL_RECYCLE (í…ŒìŠ¤íŠ¸ìš©)
_DEFAULT_POOL_RECYCLE = 60 * 15  # 15ë¶„ìœ¼ë¡œ ë‹¨ì¶• (AWS Redshift idle timeout ëŒ€ì‘)
_TEST_POOL_RECYCLE = None  # í…ŒìŠ¤íŠ¸ìš© ì˜¤ë²„ë¼ì´ë“œ

def get_pool_recycle_time():
    """í˜„ì¬ POOL_RECYCLE ì‹œê°„ì„ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ê³ ë ¤)"""
    if _TEST_POOL_RECYCLE is not None:
        return _TEST_POOL_RECYCLE
    return _DEFAULT_POOL_RECYCLE

def set_test_pool_recycle(seconds):
    """í…ŒìŠ¤íŠ¸ìš© POOL_RECYCLE ì‹œê°„ ì„¤ì •"""
    global _TEST_POOL_RECYCLE
    _TEST_POOL_RECYCLE = seconds
    logger.info(f"ğŸ§ª TEST: POOL_RECYCLE set to {seconds}s ({seconds/60:.1f} minutes)")

def reset_pool_recycle():
    """POOL_RECYCLEì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›"""
    global _TEST_POOL_RECYCLE
    _TEST_POOL_RECYCLE = None
    logger.info(f"ğŸ”„ POOL_RECYCLE reset to default {_DEFAULT_POOL_RECYCLE}s")

POOL_RECYCLE = get_pool_recycle_time()  # ë™ì ìœ¼ë¡œ ê³„ì‚°ë¨
POOL_SIZE_MAP = {"development": 3, "production": 10, "dev": 3, "prod": 10, "local": 2}
TIMEOUT = 20  # ì—°ê²° ëŒ€ê¸°ì‹œê°„ ë” ë‹¨ì¶•
MAX_RETRIES = 3  # ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜


def get_environment_pool_size():
    """í™˜ê²½ë³„ Pool Sizeë¥¼ ì•ˆì „í•˜ê²Œ ê°ì§€"""
    env = get_secret("ENVIRONMENT")
    logger.info(f"ğŸ” Detected environment: '{env}'")
    
    if env in POOL_SIZE_MAP:
        size = POOL_SIZE_MAP[env]
    else:
        # ê¸°ë³¸ê°’ì„ ë” ì•ˆì „í•˜ê²Œ ì„¤ì •
        env_lower = str(env).lower() if env else "unknown"
        if any(keyword in env_lower for keyword in ['dev', 'development']):
            size = 5
        elif any(keyword in env_lower for keyword in ['prod', 'production']):
            size = 20
        else:
            size = 3  # ì•ˆì „í•œ ê¸°ë³¸ê°’
        logger.warning(f"Unknown environment '{env}', using default pool size: {size}")
    
    logger.info(f"âœ… Pool size set to: {size}")
    return size


POOL_SIZE = get_environment_pool_size()


# Redshift Connection
def redshift_connect():
    conn = redshift_connector.connect(
        host=get_secret("REDSHIFT_HOST"),
        database=get_secret("REDSHIFT_DBNAME"),
        port=get_secret("REDSHIFT_PORT"),
        user=get_secret("REDSHIFT_USERNAME"),
        password=get_secret("REDSHIFT_PASSWORD"),
    )
    conn._created_at = time.time()  # Save creation time
    return conn


# Recycle connection if it exceeds POOL_RECYCLE (ë™ì  ê³„ì‚°)
def recycle_wrapper(conn):
    current_recycle_time = get_pool_recycle_time()
    age = time.time() - getattr(conn, "_created_at", 0)
    if age > current_recycle_time:
        logger.warning(f"Connection exceeded POOL_RECYCLE ({age:.0f}s > {current_recycle_time}s). Closing individual connection.")
        try:
            conn.close()  # ê°œë³„ ì—°ê²°ë§Œ ë‹«ê¸° (í’€ì€ ìœ ì§€)
        except Exception as e:
            logger.error(f"Error closing expired connection: {e}")
        raise Exception("Expired connection closed. Pool maintained.")
    return conn


# Actual connection creator (includes recycle check)
def redshift_connect_recycled():
    conn = redshift_connect()
    return recycle_wrapper(conn)


# Create an optimized SQLAlchemy QueuePool for AWS Redshift
redshift_pool = QueuePool(
    redshift_connect,  # ê¸°ë³¸ ì—°ê²° í•¨ìˆ˜ (ìš°ë¦¬ì˜ ì»¤ìŠ¤í…€ ì¬í™œìš© ë¡œì§ ì‚¬ìš©)
    max_overflow=5,  # ë‚®ì¶¤: poolì´ ì°¨ë©´ ë¹ ë¥´ê²Œ fresh connection ìƒì„±
    pool_size=POOL_SIZE,
    timeout=TIMEOUT,
    # Note: pool_recycleê³¼ pool_pre_pingì€ ì´ ë²„ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
    # ëŒ€ì‹  ìš°ë¦¬ì˜ recycle_wrapperì™€ validation í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬
)


# Enhanced Redshift connection validation with retry logic
async def validate_redshift_connection(conn):
    """Enhanced connection validation with proper cleanup and timeout."""
    cursor = None
    try:
        # Quick timeout for validation to prevent hanging
        cursor = conn.cursor()
        await asyncio.wait_for(
            asyncio.to_thread(cursor.execute, "SELECT 1"), 
            timeout=5.0  # 5ì´ˆ validation timeout
        )
        result = await asyncio.to_thread(cursor.fetchone)
        return result is not None
    except asyncio.TimeoutError:
        logger.error("Connection validation timeout - connection may be stale")
        return False
    except redshift_connector.Error as e:
        logger.error(f"Redshift connection validation failed: {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected validation error: {e}")
        return False
    finally:
        # âœ… Critical: ì»¤ì„œ ì •ë¦¬ë¡œ semaphore ëˆ„ìˆ˜ ë°©ì§€
        if cursor:
            try:
                cursor.close()
            except Exception as e:
                logger.debug(f"Cursor cleanup error (non-critical): {e}")


# Robust connection creation with retry logic
async def create_fresh_connection(retries=MAX_RETRIES):
    """Create a new connection with retry logic for network issues."""
    for attempt in range(retries):
        try:
            logger.info(f"ğŸ”— Creating fresh connection (attempt {attempt + 1}/{retries})")
            conn = await asyncio.wait_for(
                asyncio.to_thread(redshift_connect),
                timeout=10.0  # 10ì´ˆ connection timeout
            )
            
            # Validate the new connection
            if await validate_redshift_connection(conn):
                logger.info("âœ… Fresh connection created and validated")
                return conn
            else:
                logger.warning("âŒ New connection failed validation, closing")
                conn.close()
                
        except asyncio.TimeoutError:
            logger.warning(f"Connection timeout on attempt {attempt + 1}")
        except Exception as e:
            logger.error(f"Connection creation failed on attempt {attempt + 1}: {e}")
        
        # Wait before retry (exponential backoff)
        if attempt < retries - 1:
            wait_time = 2 ** attempt  # 1s, 2s, 4s
            logger.info(f"â³ Waiting {wait_time}s before retry...")
            await asyncio.sleep(wait_time)
    
    raise Exception(f"Failed to create connection after {retries} attempts")


# Enhanced Redshift connection with robust error handling
async def get_redshift_connection():
    conn = None
    fresh_connection_created = False
    
    try:
        # Try to get connection from pool first
        try:
            conn = await asyncio.wait_for(
                asyncio.to_thread(redshift_pool.connect), 
                timeout=10.0
            )
            
            # Check if connection needs recycling
            try:
                recycle_wrapper(conn)
            except Exception as e:
                if "Expired connection closed" in str(e):
                    logger.info("â™»ï¸ Connection expired, creating fresh one")
                    if conn:
                        try:
                            conn.close()
                        except:
                            pass
                    # Create fresh connection with retry logic
                    conn = await create_fresh_connection()
                    fresh_connection_created = True
                else:
                    raise e
            
            # Validate connection if not fresh (fresh connections are already validated)
            if not fresh_connection_created:
                if not await validate_redshift_connection(conn):
                    logger.warning("â™»ï¸ Pool connection failed validation, creating fresh one")
                    conn.close()
                    conn = await create_fresh_connection()
                    fresh_connection_created = True
                    
        except asyncio.TimeoutError:
            logger.warning("Pool connection timeout, creating fresh connection")
            conn = await create_fresh_connection()
            fresh_connection_created = True
            
        yield conn
        
    except Exception as e:
        logger.error(f"Error acquiring Redshift connection: {type(e).__name__}: {e}")
        if conn:
            try:
                conn.close()
            except:
                pass
                
        # Determine appropriate HTTP status code
        if "timeout" in str(e).lower():
            raise HTTPException(status_code=504, detail="Database connection timeout")
        elif "network" in str(e).lower() or "socket" in str(e).lower():
            raise HTTPException(status_code=503, detail="Database connection unavailable")
        else:
            raise HTTPException(status_code=500, detail="Database connection error")
            
    finally:
        # Enhanced cleanup
        if conn:
            try:
                # Clean up any cursors
                if hasattr(conn, '_cursors') and conn._cursors:
                    for cursor in list(conn._cursors):
                        try:
                            cursor.close()
                        except:
                            pass
                conn.close()
                if fresh_connection_created:
                    logger.debug("ğŸ§¹ Fresh connection cleaned up")
            except Exception as e:
                logger.error(f"Error in connection cleanup: {e}")


# Connection pool reference for lifespan management
redshift_connection_pool = redshift_pool


# Enhanced pool status and monitoring functions
def get_pool_status() -> dict:
    """Get current pool status for debugging/monitoring"""
    try:
        status = {
            "pool_size": redshift_pool.size(),
            "checked_in": redshift_pool.checkedin(),
            "checked_out": redshift_pool.checkedout(), 
            "overflow": redshift_pool.overflow(),
            "pool_recycle_seconds": POOL_RECYCLE,
            "test_mode": TEST_MODE if 'TEST_MODE' in globals() else False,
            "total_connections": redshift_pool.size() + redshift_pool.overflow()
        }
        
        # ì—°ê²° í’€ ê±´ê°• ìƒíƒœ í‰ê°€
        total_capacity = status["pool_size"] + 5  # max_overflow
        utilization = (status["checked_out"] + status["overflow"]) / total_capacity * 100
        
        status["utilization_percent"] = round(utilization, 1)
        status["health_status"] = "healthy" if utilization < 80 else "warning" if utilization < 95 else "critical"
        
        return status
    except Exception as e:
        logger.error(f"Error getting pool status: {e}")
        return {"error": str(e)}


def log_pool_metrics():
    """24/7 ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í’€ ìƒíƒœ ë¡œê¹…"""
    status = get_pool_status()
    if "error" not in status:
        logger.info(f"ğŸ“Š Pool Metrics: size={status['pool_size']}, "
                   f"active={status['checked_out']}, idle={status['checked_in']}, "
                   f"overflow={status['overflow']}, utilization={status['utilization_percent']}%, "
                   f"health={status['health_status']}")
    else:
        logger.error(f"Failed to get pool metrics: {status['error']}")


# Test connection with forced aging
async def test_connection_with_aging():
    """í…ŒìŠ¤íŠ¸ìš©: ì—°ê²° ìƒì„± í›„ ê°•ì œë¡œ ë§Œë£Œì‹œì¼œ ì¬í™œìš© ë¡œì§ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª Testing connection aging and recycling logic...")
        
        # 1. ì •ìƒ ì—°ê²° íšë“
        conn = await asyncio.to_thread(redshift_pool.connect)
        logger.info(f"âœ… Initial connection acquired at {time.time()}")
        
        # 2. ì—°ê²° ì‹œê°„ì„ ê³¼ê±°ë¡œ ì¡°ì‘
        old_time = time.time() - POOL_RECYCLE - 10  # 10ì´ˆ ë” ê³¼ê±°
        conn._created_at = old_time
        logger.info(f"ğŸ§ª Connection age manipulated to {old_time} (expired)")
        
        # 3. ì—°ê²° ë°˜ë‚© (í’€ë¡œ ë˜ëŒë¦¼)
        conn.close()
        
        # 4. ë‹¤ì‹œ ì—°ê²° íšë“ ì‹œë„ (ì¬í™œìš© ë¡œì§ ì‹¤í–‰)
        logger.info("ğŸ§ª Attempting to get connection again (should trigger recycle)")
        
        test_conn = None
        recycle_triggered = False
        try:
            test_conn = await asyncio.to_thread(redshift_pool.connect)
            recycle_wrapper(test_conn)  # ìˆ˜ë™ìœ¼ë¡œ recycle ì²´í¬
            logger.info("âœ… Connection acquired after recycle check")
        except Exception as e:
            if "Expired connection closed" in str(e):
                recycle_triggered = True
                logger.info("âœ… Recycle logic triggered successfully!")
                # ì¬í™œìš© í›„ ìƒˆ ì—°ê²° ì‹œë„
                test_conn = await asyncio.to_thread(redshift_pool.connect)
                logger.info("âœ… New connection acquired after recycling")
            else:
                raise e
        finally:
            if test_conn:
                test_conn.close()
        
        return {
            "message": "Connection aging test completed",
            "recycle_triggered": recycle_triggered,
            "pool_status": get_pool_status()
        }
        
    except Exception as e:
        logger.error(f"Connection aging test failed: {e}")
        return {
            "message": "Connection aging test failed",
            "error": str(e),
            "pool_status": get_pool_status()
        }


# ============================================================
# ì¶”ê°€ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜ë“¤ (ì¦‰ì‹œ í…ŒìŠ¤íŠ¸ìš©)

async def test_connection_breakdown():
    """ì—°ê²°ì„ ê°•ì œë¡œ ëŠì–´ì„œ ë³µêµ¬ ë¡œì§ í…ŒìŠ¤íŠ¸"""
    try:
        logger.info("ğŸ§ª Testing connection breakdown and recovery...")
        
        # 1. ì •ìƒ ì—°ê²° íšë“
        conn = await asyncio.to_thread(redshift_pool.connect)
        logger.info("âœ… Normal connection acquired")
        
        # 2. ì—°ê²°ì„ ê°•ì œë¡œ ë‹«ê¸° (broken pipe ì‹œë®¬ë ˆì´ì…˜)
        try:
            conn.close()
            logger.info("ğŸ”Œ Connection forcibly closed")
        except Exception as e:
            logger.debug(f"Error closing connection: {e}")
        
        # 3. ìƒˆ ì—°ê²° ì‹œë„ (ë³µêµ¬ í…ŒìŠ¤íŠ¸)
        logger.info("ğŸ”„ Attempting to get new connection...")
        recovery_start = time.time()
        
        try:
            new_conn = None
            async with get_redshift_connection() as test_conn:
                new_conn = test_conn
                cursor = test_conn.cursor()
                cursor.execute("SELECT 'Recovery successful' as message")
                result = cursor.fetchone()
                cursor.close()
                
                recovery_time = time.time() - recovery_start
                logger.info(f"âœ… Connection recovered in {recovery_time:.2f}s")
                
                return {
                    "message": "Connection breakdown and recovery test completed",
                    "recovery_successful": True,
                    "recovery_time_seconds": round(recovery_time, 2),
                    "test_result": result[0] if result else None,
                    "pool_status": get_pool_status()
                }
                
        except Exception as e:
            recovery_time = time.time() - recovery_start
            logger.error(f"âŒ Recovery failed: {e}")
            return {
                "message": "Connection recovery test failed",
                "recovery_successful": False,
                "recovery_time_seconds": round(recovery_time, 2),
                "error": str(e),
                "pool_status": get_pool_status()
            }
        
    except Exception as e:
        logger.error(f"Connection breakdown test failed: {e}")
        return {
            "message": "Connection breakdown test failed",
            "error": str(e),
            "pool_status": get_pool_status()
        }


async def test_pool_exhaustion(concurrent_requests=10):
    """Pool exhaustion í…ŒìŠ¤íŠ¸ - ë™ì‹œì— ë§ì€ ì—°ê²° ìš”ì²­"""
    try:
        logger.info(f"ğŸ§ª Testing pool exhaustion with {concurrent_requests} concurrent requests...")
        
        start_time = time.time()
        results = []
        
        async def single_connection_test(request_id):
            try:
                request_start = time.time()
                async with get_redshift_connection() as conn:
                    cursor = conn.cursor()
                    # ì§§ì€ ì¿¼ë¦¬ ì‹¤í–‰
                    cursor.execute(f"SELECT {request_id} as request_id, CURRENT_TIMESTAMP")
                    result = cursor.fetchone()
                    cursor.close()
                    
                    request_time = time.time() - request_start
                    return {
                        "request_id": request_id,
                        "success": True,
                        "time": round(request_time, 2),
                        "result": str(result[1]) if result else None
                    }
            except Exception as e:
                request_time = time.time() - request_start
                return {
                    "request_id": request_id,
                    "success": False,
                    "time": round(request_time, 2),
                    "error": str(e)
                }
        
        # ëª¨ë“  ìš”ì²­ì„ ë™ì‹œì— ì‹¤í–‰
        tasks = [single_connection_test(i) for i in range(concurrent_requests)]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ê²°ê³¼ ë¶„ì„
        total_time = time.time() - start_time
        successful = sum(1 for r in results if isinstance(r, dict) and r.get('success'))
        failed = len(results) - successful
        avg_time = sum(r.get('time', 0) for r in results if isinstance(r, dict)) / len(results)
        
        logger.info(f"âœ… Pool exhaustion test completed: {successful}/{len(results)} successful")
        
        return {
            "message": "Pool exhaustion test completed",
            "concurrent_requests": concurrent_requests,
            "successful": successful,
            "failed": failed,
            "total_time_seconds": round(total_time, 2),
            "average_request_time": round(avg_time, 2),
            "results": results[:5],  # ì²˜ìŒ 5ê°œ ê²°ê³¼ë§Œ í‘œì‹œ
            "pool_status_after": get_pool_status()
        }
        
    except Exception as e:
        logger.error(f"Pool exhaustion test failed: {e}")
        return {
            "message": "Pool exhaustion test failed",
            "error": str(e),
            "pool_status": get_pool_status()
        }


async def test_rapid_recycle(recycle_seconds=30):
    """ë¹ ë¥¸ ì¬í™œìš© í…ŒìŠ¤íŠ¸ - POOL_RECYCLEì„ ì§§ê²Œ ì„¤ì •í•˜ê³  í…ŒìŠ¤íŠ¸"""
    try:
        logger.info(f"ğŸ§ª Testing rapid recycle with {recycle_seconds}s recycle time...")
        
        # 1. ê¸°ì¡´ POOL_RECYCLE ì €ì¥
        original_recycle = get_pool_recycle_time()
        
        # 2. í…ŒìŠ¤íŠ¸ìš© ì§§ì€ ì‹œê°„ ì„¤ì •
        set_test_pool_recycle(recycle_seconds)
        
        try:
            # 3. ì—°ê²° ìƒì„±
            conn = await asyncio.to_thread(redshift_pool.connect)
            creation_time = time.time()
            logger.info(f"ğŸ“… Connection created at {creation_time}")
            
            # 4. ì—°ê²° ë°˜ë‚© (poolë¡œ ëŒì•„ê°)
            conn.close()
            logger.info(f"ğŸ”„ Connection returned to pool")
            
            # 5. ì¬í™œìš© ì‹œê°„ê¹Œì§€ ëŒ€ê¸°
            wait_time = recycle_seconds + 5  # 5ì´ˆ ì—¬ìœ 
            logger.info(f"â³ Waiting {wait_time}s for connection to expire...")
            await asyncio.sleep(wait_time)
            
            # 6. ìƒˆ ì—°ê²° ìš”ì²­ (ì¬í™œìš© ë¡œì§ ì‹¤í–‰ë¨)
            logger.info("ğŸ”„ Requesting new connection (should trigger recycle)...")
            test_start = time.time()
            
            async with get_redshift_connection() as new_conn:
                cursor = new_conn.cursor()
                cursor.execute("SELECT 'Recycle test successful' as message")
                result = cursor.fetchone()
                cursor.close()
                
                test_time = time.time() - test_start
                logger.info(f"âœ… Rapid recycle test completed in {test_time:.2f}s")
                
                return {
                    "message": "Rapid recycle test completed",
                    "recycle_seconds": recycle_seconds,
                    "wait_time": wait_time,
                    "test_time_seconds": round(test_time, 2),
                    "test_result": result[0] if result else None,
                    "recycle_triggered": True,  # ì¬í™œìš©ì´ ì‹¤í–‰ë˜ì—ˆë‹¤ê³  ê°€ì •
                    "pool_status": get_pool_status()
                }
                
        finally:
            # 7. ì›ë˜ POOL_RECYCLE ë³µì›
            reset_pool_recycle()
            logger.info(f"ğŸ”„ POOL_RECYCLE restored to {original_recycle}s")
        
    except Exception as e:
        # ì—ëŸ¬ ë°œìƒ ì‹œë„ ì›ë˜ ì„¤ì • ë³µì›
        reset_pool_recycle()
        logger.error(f"Rapid recycle test failed: {e}")
        return {
            "message": "Rapid recycle test failed",
            "error": str(e),
            "pool_status": get_pool_status()
        }


# Initialize Redshift connection pool
def initialize_redshift_pool():
    """Initialize optimized Redshift connection pool on application startup."""
    logger.info("ğŸ”— Initializing Optimized Redshift connection pool for AWS...")
    logger.info(f"Pool size: {POOL_SIZE}, Max overflow: 5, Timeout: {TIMEOUT}s")
    logger.info(f"Pool recycle: {POOL_RECYCLE}s ({POOL_RECYCLE/60:.1f} minutes)")
    logger.info(f"Max retries: {MAX_RETRIES}, Enhanced validation: True")
    logger.info("ğŸ›¡ï¸ Features: Connection aging, Retry logic, Timeout protection")
    
    # ì´ˆê¸° í’€ ìƒíƒœ ë¡œê¹…
    log_pool_metrics()
    
    if 'TEST_MODE' in globals() and TEST_MODE:
        logger.info("ğŸ§ª Running in TEST MODE - pool recycle can be overridden")
