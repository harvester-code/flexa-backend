import asyncio
import time

import redshift_connector
from fastapi import HTTPException
from loguru import logger
from sqlalchemy.pool import QueuePool

from packages.doppler.client import get_secret

# ============================================================
# NOTE: Redshift ì—°ê²°ì„ ìœ„í•œ ì„¤ì • (Optimized for AWS Redshift idle timeouts)

# ë™ì ìœ¼ë¡œ ì¡°ì • ê°€ëŠ¥í•œ POOL_RECYCLE (í…ŒìŠ¤íŠ¸ìš©)
_DEFAULT_POOL_RECYCLE = 60 * 15  # 15ë¶„ìœ¼ë¡œ ë‹¨ì¶• (AWS Redshift idle timeout ëŒ€ì‘)
_TEST_POOL_RECYCLE = None  # í…ŒìŠ¤íŠ¸ìš© ì˜¤ë²„ë¼ì´ë“œ

def get_pool_recycle_time():
    """í˜„ì¬ POOL_RECYCLE ì‹œê°„ì„ ë°˜í™˜ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ ê³ ë ¤)"""
    if _TEST_POOL_RECYCLE is not None:
        return _TEST_POOL_RECYCLE
    return _DEFAULT_POOL_RECYCLE

def set_test_pool_recycle(seconds):
    """í…ŒìŠ¤íŠ¸ìš© POOL_RECYCLE ì‹œê°„ ì„¤ì •"""
    global _TEST_POOL_RECYCLE
    _TEST_POOL_RECYCLE = seconds
    logger.info(f"ğŸ§ª TEST: POOL_RECYCLE set to {seconds}s ({seconds/60:.1f} minutes)")

def reset_pool_recycle():
    """POOL_RECYCLEì„ ê¸°ë³¸ê°’ìœ¼ë¡œ ë³µì›"""
    global _TEST_POOL_RECYCLE
    _TEST_POOL_RECYCLE = None
    logger.info(f"ğŸ”„ POOL_RECYCLE reset to default {_DEFAULT_POOL_RECYCLE}s")

POOL_RECYCLE = get_pool_recycle_time()  # ë™ì ìœ¼ë¡œ ê³„ì‚°ë¨
POOL_SIZE_MAP = {"development": 3, "production": 10, "dev": 3, "stg": 5, "prod": 10}
TIMEOUT = 20  # ì—°ê²° ëŒ€ê¸°ì‹œê°„ ë” ë‹¨ì¶•
MAX_RETRIES = 3  # ì—°ê²° ì‹¤íŒ¨ ì‹œ ì¬ì‹œë„ íšŸìˆ˜


def get_environment_pool_size():
    """í™˜ê²½ë³„ Pool Sizeë¥¼ ì•ˆì „í•˜ê²Œ ê°ì§€"""
    env = get_secret("DOPPLER_ENVIRONMENT")
    logger.info(f"ğŸ” Detected environment: '{env}'")
    
    if env in POOL_SIZE_MAP:
        size = POOL_SIZE_MAP[env]
    else:
        # ê¸°ë³¸ê°’ì„ ë” ì•ˆì „í•˜ê²Œ ì„¤ì •
        env_lower = str(env).lower() if env else "unknown"
        if any(keyword in env_lower for keyword in ['dev', 'development']):
            size = 5
        elif any(keyword in env_lower for keyword in ['prod', 'production']):
            size = 20
        else:
            size = 3  # ì•ˆì „í•œ ê¸°ë³¸ê°’
        logger.warning(f"Unknown environment '{env}', using default pool size: {size}")
    
    logger.info(f"âœ… Pool size set to: {size}")
    return size


POOL_SIZE = get_environment_pool_size()


# Redshift Connection
def redshift_connect():
    conn = redshift_connector.connect(
        host=get_secret("REDSHIFT_HOST"),
        database=get_secret("REDSHIFT_DBNAME"),
        port=get_secret("REDSHIFT_PORT"),
        user=get_secret("REDSHIFT_USERNAME"),
        password=get_secret("REDSHIFT_PASSWORD"),
    )
    conn._created_at = time.time()  # Save creation time
    return conn


# Recycle connection if it exceeds POOL_RECYCLE (ë™ì  ê³„ì‚°)
def recycle_wrapper(conn):
    current_recycle_time = get_pool_recycle_time()
    age = time.time() - getattr(conn, "_created_at", 0)
    if age > current_recycle_time:
        logger.warning(f"Connection exceeded POOL_RECYCLE ({age:.0f}s > {current_recycle_time}s). Closing individual connection.")
        try:
            conn.close()  # ê°œë³„ ì—°ê²°ë§Œ ë‹«ê¸° (í’€ì€ ìœ ì§€)
        except Exception as e:
            logger.error(f"Error closing expired connection: {e}")
        raise Exception("Expired connection closed. Pool maintained.")
    return conn


# Create an optimized SQLAlchemy QueuePool for AWS Redshift
redshift_pool = QueuePool(
    redshift_connect,  # ê¸°ë³¸ ì—°ê²° í•¨ìˆ˜ (ìš°ë¦¬ì˜ ì»¤ìŠ¤í…€ ì¬í™œìš© ë¡œì§ ì‚¬ìš©)
    max_overflow=5,  # ë‚®ì¶¤: poolì´ ì°¨ë©´ ë¹ ë¥´ê²Œ fresh connection ìƒì„±
    pool_size=POOL_SIZE,
    timeout=TIMEOUT,
    # Note: pool_recycleê³¼ pool_pre_pingì€ ì´ ë²„ì „ì—ì„œ ì§€ì›ë˜ì§€ ì•ŠìŒ
    # ëŒ€ì‹  ìš°ë¦¬ì˜ recycle_wrapperì™€ validation í•¨ìˆ˜ì—ì„œ ì²˜ë¦¬
)


# Enhanced Redshift connection validation with retry logic
async def validate_redshift_connection(conn):
    """Enhanced connection validation with proper cleanup and timeout."""
    cursor = None
    try:
        # Quick timeout for validation to prevent hanging
        cursor = conn.cursor()
        await asyncio.wait_for(
            asyncio.to_thread(cursor.execute, "SELECT 1"), 
            timeout=5.0  # 5ì´ˆ validation timeout
        )
        result = await asyncio.to_thread(cursor.fetchone)
        return result is not None
    except asyncio.TimeoutError:
        logger.error("Connection validation timeout - connection may be stale")
        return False
    except redshift_connector.Error as e:
        logger.error(f"Redshift connection validation failed: {e}")
        return False
    except Exception as e:
        logger.error(f"Unexpected validation error: {e}")
        return False
    finally:
        # âœ… Critical: ì»¤ì„œ ì •ë¦¬ë¡œ semaphore ëˆ„ìˆ˜ ë°©ì§€
        if cursor:
            try:
                cursor.close()
            except Exception as e:
                logger.debug(f"Cursor cleanup error (non-critical): {e}")


# Robust connection creation with retry logic
async def create_fresh_connection(retries=MAX_RETRIES):
    """Create a new connection with retry logic for network issues."""
    for attempt in range(retries):
        try:
            logger.info(f"ğŸ”— Creating fresh connection (attempt {attempt + 1}/{retries})")
            conn = await asyncio.wait_for(
                asyncio.to_thread(redshift_connect),
                timeout=10.0  # 10ì´ˆ connection timeout
            )
            
            # Validate the new connection
            if await validate_redshift_connection(conn):
                logger.info("âœ… Fresh connection created and validated")
                return conn
            else:
                logger.warning("âŒ New connection failed validation, closing")
                conn.close()
                
        except asyncio.TimeoutError:
            logger.warning(f"Connection timeout on attempt {attempt + 1}")
        except Exception as e:
            logger.error(f"Connection creation failed on attempt {attempt + 1}: {e}")
        
        # Wait before retry (exponential backoff)
        if attempt < retries - 1:
            wait_time = 2 ** attempt  # 1s, 2s, 4s
            logger.info(f"â³ Waiting {wait_time}s before retry...")
            await asyncio.sleep(wait_time)
    
    raise Exception(f"Failed to create connection after {retries} attempts")


# Enhanced Redshift connection with robust error handling
async def get_redshift_connection():
    conn = None
    fresh_connection_created = False
    
    try:
        # Try to get connection from pool first
        try:
            conn = await asyncio.wait_for(
                asyncio.to_thread(redshift_pool.connect), 
                timeout=10.0
            )
            
            # Check if connection needs recycling
            try:
                recycle_wrapper(conn)
            except Exception as e:
                if "Expired connection closed" in str(e):
                    logger.info("â™»ï¸ Connection expired, creating fresh one")
                    if conn:
                        try:
                            conn.close()
                        except:
                            pass
                    # Create fresh connection with retry logic
                    conn = await create_fresh_connection()
                    fresh_connection_created = True
                else:
                    raise e
            
            # Validate connection if not fresh (fresh connections are already validated)
            if not fresh_connection_created:
                if not await validate_redshift_connection(conn):
                    logger.warning("â™»ï¸ Pool connection failed validation, creating fresh one")
                    conn.close()
                    conn = await create_fresh_connection()
                    fresh_connection_created = True
                    
        except asyncio.TimeoutError:
            logger.warning("Pool connection timeout, creating fresh connection")
            conn = await create_fresh_connection()
            fresh_connection_created = True
            
        yield conn
        
    except Exception as e:
        logger.error(f"Error acquiring Redshift connection: {type(e).__name__}: {e}")
        if conn:
            try:
                conn.close()
            except:
                pass
                
        # Determine appropriate HTTP status code
        if "timeout" in str(e).lower():
            raise HTTPException(status_code=504, detail="Database connection timeout")
        elif "network" in str(e).lower() or "socket" in str(e).lower():
            raise HTTPException(status_code=503, detail="Database connection unavailable")
        else:
            raise HTTPException(status_code=500, detail="Database connection error")
            
    finally:
        # Enhanced cleanup
        if conn:
            try:
                # Clean up any cursors
                if hasattr(conn, '_cursors') and conn._cursors:
                    for cursor in list(conn._cursors):
                        try:
                            cursor.close()
                        except:
                            pass
                conn.close()
                if fresh_connection_created:
                    logger.debug("ğŸ§¹ Fresh connection cleaned up")
            except Exception as e:
                logger.error(f"Error in connection cleanup: {e}")


# Connection pool reference for lifespan management
redshift_connection_pool = redshift_pool


# Enhanced pool status and monitoring functions
def get_pool_status() -> dict:
    """Get current pool status for debugging/monitoring"""
    try:
        status = {
            "pool_size": redshift_pool.size(),
            "checked_in": redshift_pool.checkedin(),
            "checked_out": redshift_pool.checkedout(), 
            "overflow": redshift_pool.overflow(),
            "pool_recycle_seconds": POOL_RECYCLE,
            "total_connections": redshift_pool.size() + redshift_pool.overflow()
        }
        
        # ì—°ê²° í’€ ê±´ê°• ìƒíƒœ í‰ê°€
        total_capacity = status["pool_size"] + 5  # max_overflow
        utilization = (status["checked_out"] + status["overflow"]) / total_capacity * 100
        
        status["utilization_percent"] = round(utilization, 1)
        status["health_status"] = "healthy" if utilization < 80 else "warning" if utilization < 95 else "critical"
        
        return status
    except Exception as e:
        logger.error(f"Error getting pool status: {e}")
        return {"error": str(e)}


def log_pool_metrics():
    """24/7 ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í’€ ìƒíƒœ ë¡œê¹…"""
    status = get_pool_status()
    if "error" not in status:
        logger.info(f"ğŸ“Š Pool Metrics: size={status['pool_size']}, "
                   f"active={status['checked_out']}, idle={status['checked_in']}, "
                   f"overflow={status['overflow']}, utilization={status['utilization_percent']}%, "
                   f"health={status['health_status']}")
    else:
        logger.error(f"Failed to get pool metrics: {status['error']}")



# Initialize Redshift connection pool
def initialize_redshift_pool():
    """Initialize optimized Redshift connection pool on application startup."""
    logger.info("ğŸ”— Initializing Optimized Redshift connection pool for AWS...")
    logger.info(f"Pool size: {POOL_SIZE}, Max overflow: 5, Timeout: {TIMEOUT}s")
    logger.info(f"Pool recycle: {POOL_RECYCLE}s ({POOL_RECYCLE/60:.1f} minutes)")
    logger.info(f"Max retries: {MAX_RETRIES}, Enhanced validation: True")
    logger.info("ğŸ›¡ï¸ Features: Connection aging, Retry logic, Timeout protection")
    
    # ì´ˆê¸° í’€ ìƒíƒœ ë¡œê¹…
    log_pool_metrics()
