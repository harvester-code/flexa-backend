from typing import List, Optional

import pandas as pd
from loguru import logger

from app.routes.home.domain.repository import IHomeRepository
from packages.aws.s3.s3_manager import S3Manager


class HomeRepository(IHomeRepository):
    def __init__(self, s3_manager: S3Manager):
        self.s3_manager = s3_manager

    async def load_simulation_parquet(self, scenario_id: str) -> Optional[pd.DataFrame]:
        return await self.s3_manager.get_parquet_async(
            scenario_id, "simulation-pax.parquet"
        )

    async def load_metadata(self, scenario_id: str, filename: str) -> Optional[dict]:
        return await self.s3_manager.get_json_async(scenario_id=scenario_id, filename=filename)

    async def is_cache_valid(self, scenario_id: str, cache_filename: str) -> bool:
        """ìºì‹œê°€ ìœ íš¨í•œì§€ í™•ì¸ (simulation-pax.parquet ìˆ˜ì •ì¼ê³¼ ë¹„êµ)
        
        ìºì‹œê°€ parquet íŒŒì¼ë³´ë‹¤ ìµœì‹ ì´ë©´ ìœ íš¨, ì˜¤ë˜ë˜ì—ˆìœ¼ë©´ ë¬´íš¨
        """
        logger.info(f"  ğŸ” [REPO] Checking cache metadata for: {cache_filename}")
        
        # 1. ìºì‹œ íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        cache_metadata = await self.s3_manager.get_metadata_async(scenario_id, cache_filename)
        if not cache_metadata:
            logger.info(f"  âŒ [REPO] Cache file NOT FOUND in S3: {cache_filename}")
            return False
        
        logger.info(f"  âœ… [REPO] Cache file exists in S3")
        
        # 2. parquet íŒŒì¼ ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        parquet_metadata = await self.s3_manager.get_metadata_async(scenario_id, "simulation-pax.parquet")
        if not parquet_metadata:
            logger.warning(f"  âš ï¸ [REPO] Parquet file not found for scenario_id={scenario_id}")
            return False
        
        cache_modified = cache_metadata.get('last_modified')
        parquet_modified = parquet_metadata.get('last_modified')
        
        if not cache_modified or not parquet_modified:
            logger.warning(f"  âš ï¸ [REPO] Missing modification timestamps")
            return False
        
        # 3. íƒ€ì„ìŠ¤íƒ¬í”„ ë¹„êµ: ìºì‹œê°€ parquetë³´ë‹¤ ìµœì‹ ì´ë©´ ìœ íš¨
        is_valid = cache_modified > parquet_modified
        
        if is_valid:
            logger.info(f"  âœ… [REPO] Cache is NEWER than parquet")
            logger.info(f"      ğŸ“… Cache modified:   {cache_modified}")
            logger.info(f"      ğŸ“… Parquet modified: {parquet_modified}")
        else:
            logger.info(f"  âŒ [REPO] Cache is OLDER than parquet (needs refresh)")
            logger.info(f"      ğŸ“… Cache modified:   {cache_modified}")
            logger.info(f"      ğŸ“… Parquet modified: {parquet_modified}")
        
        return is_valid

    async def load_cached_response(self, scenario_id: str, cache_filename: str) -> Optional[dict]:
        """ìºì‹œëœ ì‘ë‹µ ë¡œë“œ"""
        logger.info(f"  ğŸ“¥ [REPO] Loading cached JSON from S3...")
        cached_data = await self.s3_manager.get_json_async(scenario_id, cache_filename)
        if cached_data:
            logger.info(f"  âœ… [REPO] Successfully loaded cached response")
        else:
            logger.warning(f"  âš ï¸ [REPO] Failed to load cached response")
        return cached_data

    async def save_cached_response(self, scenario_id: str, cache_filename: str, data: dict) -> bool:
        """ê³„ì‚°ëœ ì‘ë‹µì„ ìºì‹œì— ì €ì¥"""
        logger.info(f"  ğŸ’¾ [REPO] Saving computed result to S3: {cache_filename}")
        success = await self.s3_manager.save_json_async(scenario_id, cache_filename, data)
        if success:
            logger.info(f"  âœ… [REPO] Cache saved successfully to S3")
        else:
            logger.error(f"  âŒ [REPO] FAILED to save cache to S3")
        return success

    async def delete_old_caches(self, scenario_id: str, prefix: str, keep_filename: str) -> List[str]:
        """í˜„ì¬ ë²„ì „ì„ ì œì™¸í•œ ì´ì „ ìºì‹œ íŒŒì¼ ì‚­ì œ
        
        Args:
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID
            prefix: ìºì‹œ íŒŒì¼ prefix (ì˜ˆ: "home-static-response-")
            keep_filename: ì‚­ì œí•˜ì§€ ì•Šì„ í˜„ì¬ ìºì‹œ íŒŒì¼ëª…
            
        Returns:
            ì‚­ì œëœ íŒŒì¼ëª… ë¦¬ìŠ¤íŠ¸
        """
        all_files = await self.s3_manager.list_files_async(scenario_id)
        old_caches = [
            f for f in all_files
            if f.startswith(prefix) and f != keep_filename
        ]
        
        deleted = []
        for old_file in old_caches:
            success = await self.s3_manager.delete_json_async(scenario_id, old_file)
            if success:
                deleted.append(old_file)
                logger.info(f"  ğŸ—‘ï¸ [REPO] Deleted old cache: {old_file}")
            else:
                logger.warning(f"  âš ï¸ [REPO] Failed to delete old cache: {old_file}")
        
        return deleted
