from dependency_injector.wiring import Provide, inject
from fastapi import APIRouter, Depends, status, HTTPException
from loguru import logger

from app.libs.containers import Container
from packages.supabase.dependencies import verify_token
from app.routes.ai_agent.application.service import AIAgentService
from app.routes.ai_agent.application.core import CommandParser, CommandExecutor
from app.routes.ai_agent.interface.schema import (
    ChatRequest, 
    ChatLocalRequest, 
    ChatResponse,
    CommandRequest,
    CommandResponse
)


ai_agent_router = APIRouter(
    prefix="/ai-agent",
    dependencies=[Depends(verify_token)],
)


@ai_agent_router.post(
    "/chat",
    status_code=status.HTTP_200_OK,
    response_model=ChatResponse,
    summary="ì™¸ë¶€ OpenAI APIì™€ ì±„íŒ…",
    description="ì™¸ë¶€ OpenAI APIë¥¼ í†µí•´ ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ì‚¬ìš©ìì˜ ë©”ì‹œì§€ì™€ ëŒ€í™” íˆìŠ¤í† ë¦¬ë¥¼ ì „ì†¡í•˜ë©´ AI ì‘ë‹µì„ ë°›ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.",
)
@inject
async def chat(
    request: ChatRequest,
    ai_agent_service: AIAgentService = Depends(Provide[Container.ai_agent_service]),
):
    """
    ì™¸ë¶€ OpenAI APIì™€ì˜ ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
    
    - **messages**: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœì†Œ 1ê°œ ì´ìƒì˜ ë©”ì‹œì§€ í•„ìš”)
    - **model**: ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸: gpt-4)
    - **max_tokens**: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 1024)
    - **temperature**: ì‘ë‹µì˜ ì°½ì˜ì„± (0.0 ~ 2.0, ê¸°ë³¸: 1.0)
    """
    result = await ai_agent_service.chat(
        messages=request.messages,
        model=request.model,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )
    
    # OpenAI API ì‘ë‹µì„ ìš°ë¦¬ì˜ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    return ChatResponse(
        content=result["choices"][0]["message"]["content"],
        model=result["model"],
        usage=result["usage"],
    )


@ai_agent_router.post(
    "/chat/local",
    status_code=status.HTTP_200_OK,
    response_model=ChatResponse,
    summary="ë¡œì»¬ DGX Spark AI ì„œë²„ì™€ ì±„íŒ…",
    description="ë¡œì»¬ DGX Sparkì— ë°°í¬ëœ TRT-LLM ì„œë²„(gpt-oss-120b)ë¥¼ í†µí•´ ëŒ€í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤. API í‚¤ê°€ í•„ìš” ì—†ìœ¼ë©° ë‚´ë¶€ ë„¤íŠ¸ì›Œí¬ë¡œ í†µì‹ í•©ë‹ˆë‹¤.",
)
@inject
async def chat_local(
    request: ChatLocalRequest,
    ai_agent_service: AIAgentService = Depends(Provide[Container.ai_agent_service]),
):
    """
    ë¡œì»¬ DGX Spark AI ì„œë²„ì™€ì˜ ëŒ€í™” ì—”ë“œí¬ì¸íŠ¸
    
    - **messages**: ëŒ€í™” íˆìŠ¤í† ë¦¬ (ìµœì†Œ 1ê°œ ì´ìƒì˜ ë©”ì‹œì§€ í•„ìš”)
    - **model**: ì‚¬ìš©í•  ë¡œì»¬ ëª¨ë¸ (ê¸°ë³¸: openai/gpt-oss-120b)
    - **max_tokens**: ìƒì„±í•  ìµœëŒ€ í† í° ìˆ˜ (ê¸°ë³¸: 1024, ìµœëŒ€: 32768)
    - **temperature**: ì‘ë‹µì˜ ì°½ì˜ì„± (0.0 ~ 2.0, ê¸°ë³¸: 1.0)
    """
    result = await ai_agent_service.chat_local(
        messages=request.messages,
        model=request.model,
        max_tokens=request.max_tokens,
        temperature=request.temperature,
    )
    
    # ë¡œì»¬ AI ì„œë²„ ì‘ë‹µì„ ìš°ë¦¬ì˜ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    return ChatResponse(
        content=result["choices"][0]["message"]["content"],
        model=result["model"],
        usage=result["usage"],
    )


@ai_agent_router.post(
    "/scenario/{scenario_id}/execute-command",
    status_code=status.HTTP_200_OK,
    response_model=CommandResponse,
    summary="ì‹œë‚˜ë¦¬ì˜¤ ëª…ë ¹ ì‹¤í–‰",
    description="ì‚¬ìš©ì ëª…ë ¹ì„ íŒŒì‹±í•˜ì—¬ ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •ì„ ë³€ê²½í•©ë‹ˆë‹¤. AWS S3ì˜ ë°ì´í„°ë¥¼ í™•ì¸í•˜ê³  ìˆ˜ì •í•©ë‹ˆë‹¤.",
)
@inject
async def execute_command(
    scenario_id: str,
    request: CommandRequest,
    command_parser: CommandParser = Depends(Provide[Container.command_parser]),
):
    """
    ì‹œë‚˜ë¦¬ì˜¤ ëª…ë ¹ ì‹¤í–‰ ì—”ë“œí¬ì¸íŠ¸
    
    ì‚¬ìš©ìê°€ contentë§Œ ë³´ë‚´ë©´, AIê°€ ëª…ë ¹ì„ íŒŒì‹±í•˜ê³  ì‹¤í–‰í•©ë‹ˆë‹¤.
    
    ì˜ˆì‹œ:
    - "checkin í”„ë¡œì„¸ìŠ¤ ì¶”ê°€í•´ì¤˜"
    - "ë³´ì•ˆê²€ìƒ‰ ë‹¨ê³„ ì‚­ì œí•´ì¤˜"
    - "í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ë³´ì—¬ì¤˜"
    
    - **content**: ì‚¬ìš©ì ëª…ë ¹
    - **model**: ì‚¬ìš©í•  OpenAI ëª¨ë¸ (ê¸°ë³¸: gpt-4o-2024-08-06)
    - **temperature**: ì‘ë‹µì˜ ì¼ê´€ì„± (ê¸°ë³¸: 0.1)
    """
    try:
        # 1. ëª…ë ¹ íŒŒì‹±
        parsed = await command_parser.parse_command(
            user_content=request.content,
            scenario_id=scenario_id,
            conversation_history=request.conversation_history,
            simulation_state=request.simulation_state,
            model=request.model,
            temperature=request.temperature
        )
        
        # 2. ì—ëŸ¬ ì²˜ë¦¬
        if parsed.get("action") == "error":
            return CommandResponse(
                success=False,
                message=f"ëª…ë ¹ì„ ì´í•´í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {parsed.get('error', 'Unknown error')}",
                error=parsed.get("error"),
            )
        
        # 3. ì¼ë°˜ ëŒ€í™”ì¸ ê²½ìš°
        if parsed.get("action") == "chat":
            return CommandResponse(
                success=True,
                message=parsed.get("content", ""),
                action="chat",
            )
        
        # 4. ëª…ë ¹ ì‹¤í–‰
        action = parsed.get("action")
        parameters = parsed.get("parameters", {})
        executor = command_parser.command_executor
        
        if action == "add_process":
            result = await executor.add_process(
                scenario_id=scenario_id,
                process_name=parameters.get("process_name"),
            )
            return CommandResponse(
                success=result["success"],
                message=result["message"],
                action="add_process",
                data=result.get("data"),
                error=result.get("error"),
            )
        
        elif action == "remove_process":
            result = await executor.remove_process(
                scenario_id=scenario_id,
                process_name=parameters.get("process_name"),
            )
            return CommandResponse(
                success=result["success"],
                message=result["message"],
                action="remove_process",
                data=result.get("data"),
                error=result.get("error"),
            )
        
        elif action == "list_processes":
            context = await executor.get_scenario_context(scenario_id)
            process_list = context.get("process_names", [])
            
            if not process_list:
                message = "í˜„ì¬ ì„¤ì •ëœ í”„ë¡œì„¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤."
            else:
                formatted_list = "\n".join([f"- {name}" for name in process_list])
                message = f"í˜„ì¬ í”„ë¡œì„¸ìŠ¤ ëª©ë¡ ({len(process_list)}ê°œ):\n{formatted_list}"
            
            return CommandResponse(
                success=True,
                message=message,
                action="list_processes",
                data={"processes": process_list, "count": len(process_list)},
            )
        
        elif action == "list_files":
            result = await executor.list_files(scenario_id)
            return CommandResponse(
                success=result["success"],
                message=result["message"],
                action="list_files",
                data={
                    "files": result.get("files", []),
                    "count": result.get("count", 0),
                    "categories": result.get("categories", {}),
                },
                error=result.get("error"),
            )
        
        elif action == "read_file":
            filename = parameters.get("filename")
            
            # íŒŒì¼ ì½ê¸° (ê¸°ë³¸ì ìœ¼ë¡œ summary íƒ€ì…ìœ¼ë¡œ AI ë¶„ì„)
            result = await executor.read_file(
                scenario_id=scenario_id,
                filename=filename,
                summary_type="summary"  # ê¸°ë³¸ê°’ìœ¼ë¡œ summary ì‚¬ìš©
            )
            
            if not result["success"]:
                return CommandResponse(
                    success=False,
                    message=result["message"],
                    action="read_file",
                    error=result.get("error"),
                )
            
            # AI ë¶„ì„ì´ í•„ìš”í•œ ê²½ìš°
            if result.get("needs_ai_analysis"):
                # ì›ë³¸ ì‚¬ìš©ì ì§ˆë¬¸ìœ¼ë¡œ AI ë¶„ì„ ìš”ì²­
                analysis_result = await command_parser.analyze_file_content(
                    scenario_id=scenario_id,
                    filename=filename,
                    file_content={
                        "content_preview": result.get("content_preview", ""),
                        "full_content": result.get("full_content")
                    },
                    user_query=request.content,  # ì›ë³¸ ì§ˆë¬¸
                    simulation_state=request.simulation_state,  # ğŸ‘ˆ ì‹¤ì‹œê°„ ìƒíƒœ ì „ë‹¬
                    model=request.model,
                    temperature=request.temperature
                )
                
                if analysis_result.get("success"):
                    return CommandResponse(
                        success=True,
                        message=analysis_result.get("content", ""),
                        action="read_file",
                        data={
                            "filename": filename,
                            "analysis": True,
                        },
                    )
                else:
                    return CommandResponse(
                        success=False,
                        message=f"íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {analysis_result.get('error')}",
                        action="read_file",
                        error=analysis_result.get("error"),
                    )
            
            # êµ¬ì¡°ë‚˜ ì „ì²´ ë‚´ìš©ì¸ ê²½ìš°
            return CommandResponse(
                success=result["success"],
                message=result["message"],
                action="read_file",
                data={
                    "filename": filename,
                    "structure": result.get("structure"),
                    "content": result.get("content"),
                },
                error=result.get("error"),
            )

        else:
            return CommandResponse(
                success=False,
                message=f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª…ë ¹ì…ë‹ˆë‹¤: {action}",
                error=f"Unsupported action: {action}",
            )
    
    except Exception as e:
        logger.error(f"Command execution failed: {str(e)}")
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"ëª…ë ¹ ì‹¤í–‰ ì‹¤íŒ¨: {str(e)}"
        )
