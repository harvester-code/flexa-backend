"""
ëª…ë ¹ ì‹¤í–‰ ì„œë¹„ìŠ¤ - í”„ë¡œì„¸ìŠ¤ ì¶”ê°€/ì‚­ì œ/ìˆ˜ì • ë“± ì‹¤ì œ ì‘ì—… ìˆ˜í–‰
"""
import json
import re
from typing import Dict, Any, Optional
from datetime import datetime, timezone
from loguru import logger
import pandas as pd
import numpy as np

from app.routes.simulation.application.service import SimulationService


def normalize_process_name(name: str) -> str:
    """
    í”„ë¡œì„¸ìŠ¤ ì´ë¦„ ì •ê·œí™” (í”„ë¡ íŠ¸ì—”ë“œì™€ ë™ì¼í•œ ë¡œì§)
    ì˜ˆ: "checkin" -> "check_in", "Visa-Check" -> "visa_check"
    """
    # í•œê¸€ -> ì˜ì–´ ë§¤í•‘
    korean_mapping = {
        "ì²´í¬ì¸": "check_in",
        "ë³´ì•ˆê²€ìƒ‰": "security_check",
        "ì…êµ­ì‹¬ì‚¬": "immigration",
        "ì„¸ê´€": "customs",
        "íƒ‘ìŠ¹": "boarding",
        "ë¹„ìì²´í¬": "visa_check",
        "ì—¬í–‰ì„¸": "travel_tax",
    }
    
    # í•œê¸€ ë§¤í•‘ í™•ì¸
    if name in korean_mapping:
        return korean_mapping[name]
    
    # ì˜ì–´ì¸ ê²½ìš° ì •ê·œí™”
    normalized = name.lower()
    normalized = re.sub(r'[^a-z0-9]', '_', normalized)  # ì˜ë¬¸, ìˆ«ì ì™¸ ëª¨ë“  ë¬¸ìë¥¼ ì–¸ë”ìŠ¤ì½”ì–´ë¡œ
    normalized = re.sub(r'_+', '_', normalized)  # ì—°ì†ëœ ì–¸ë”ìŠ¤ì½”ì–´ë¥¼ í•˜ë‚˜ë¡œ
    normalized = normalized.strip('_')  # ì•ë’¤ ì–¸ë”ìŠ¤ì½”ì–´ ì œê±°
    
    return normalized


class CommandExecutor:
    """ëª…ë ¹ ì‹¤í–‰ ì „ë‹´ í´ë˜ìŠ¤"""
    
    def __init__(self, simulation_service: SimulationService):
        self.simulation_service = simulation_service
    
    async def add_process(
        self, 
        scenario_id: str, 
        process_name: str,
        zones: Optional[list] = None
    ) -> Dict[str, Any]:
        """
        í”„ë¡œì„¸ìŠ¤ ì¶”ê°€
        
        Args:
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID
            process_name: í”„ë¡œì„¸ìŠ¤ ì´ë¦„ (ì •ê·œí™” ì „)
            zones: zone ëª©ë¡ (ì„ íƒì‚¬í•­)
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        try:
            # 1. í˜„ì¬ metadata ë¡œë“œ
            metadata_result = await self.simulation_service.load_scenario_metadata(scenario_id)
            metadata = metadata_result.get("metadata", {})
            
            if metadata is None:
                # ìƒˆ ì‹œë‚˜ë¦¬ì˜¤ì¸ ê²½ìš° ê¸°ë³¸ êµ¬ì¡° ìƒì„±
                metadata = {
                    "context": {
                        "scenarioId": scenario_id,
                        "airport": "",
                        "terminal": "",
                        "date": datetime.now(timezone.utc).date().isoformat(),
                        "lastSavedAt": None,
                    },
                    "flight": {
                        "selectedConditions": None,
                        "appliedFilterResult": None,
                        "total_flights": None,
                        "airlines": None,
                        "filters": None,
                    },
                    "passenger": {
                        "settings": {"min_arrival_minutes": None},
                        "pax_generation": {"rules": [], "default": {"load_factor": None, "flightCount": 0}},
                        "pax_demographics": {
                            "nationality": {"available_values": [], "rules": [], "default": {"flightCount": 0}},
                            "profile": {"available_values": [], "rules": [], "default": {"flightCount": 0}},
                        },
                        "pax_arrival_patterns": {"rules": [], "default": {"mean": None, "std": None, "flightCount": 0}},
                    },
                    "process_flow": [],
                    "terminalLayout": {"zoneAreas": {}},
                    "workflow": {
                        "currentStep": 1,
                        "step1Completed": False,
                        "step2Completed": False,
                        "availableSteps": [1],
                    },
                    "savedAt": None,
                }
            
            # 2. process_flow ê°€ì ¸ì˜¤ê¸°
            process_flow = metadata.get("process_flow", [])
            
            # 3. í”„ë¡œì„¸ìŠ¤ ì´ë¦„ ì •ê·œí™”
            normalized_name = normalize_process_name(process_name)
            
            # 4. ì¤‘ë³µ í™•ì¸
            existing_processes = [p.get("name") for p in process_flow]
            if normalized_name in existing_processes:
                return {
                    "success": False,
                    "message": f"í”„ë¡œì„¸ìŠ¤ '{process_name}'ê°€ ì´ë¯¸ ì¡´ì¬í•©ë‹ˆë‹¤.",
                    "error": f"Process '{normalized_name}' already exists",
                }
            
            # 5. ìƒˆ í”„ë¡œì„¸ìŠ¤ ìƒì„±
            new_step = {
                "step": len(process_flow),
                "name": normalized_name,
                "travel_time_minutes": 0,
                "entry_conditions": [],
                "zones": {},
            }
            
            # zonesê°€ ì œê³µëœ ê²½ìš° ì„¤ì •
            if zones and isinstance(zones, list):
                for zone_name in zones:
                    new_step["zones"][zone_name] = {
                        "facilities": []
                    }
            
            # 6. process_flowì— ì¶”ê°€
            process_flow.append(new_step)
            metadata["process_flow"] = process_flow
            
            # 7. savedAt ì—…ë°ì´íŠ¸
            metadata["savedAt"] = datetime.now(timezone.utc).isoformat()
            
            # 8. S3ì— ì €ì¥
            await self.simulation_service.save_scenario_metadata(scenario_id, metadata)
            
            logger.info(f"âœ… Process '{normalized_name}' added to scenario {scenario_id}")
            
            return {
                "success": True,
                "message": f"í”„ë¡œì„¸ìŠ¤ '{process_name}'ê°€ ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.",
                "action": "add_process",
                "data": {
                    "process": new_step,
                    "total_processes": len(process_flow),
                },
            }
        
        except Exception as e:
            logger.error(f"Failed to add process: {str(e)}")
            return {
                "success": False,
                "message": f"í”„ë¡œì„¸ìŠ¤ ì¶”ê°€ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
            }
    
    async def remove_process(
        self,
        scenario_id: str,
        process_name: str,
        step: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        í”„ë¡œì„¸ìŠ¤ ì‚­ì œ
        
        Args:
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID
            process_name: í”„ë¡œì„¸ìŠ¤ ì´ë¦„
            step: step ë²ˆí˜¸ (ì„ íƒì‚¬í•­, ì´ë¦„ìœ¼ë¡œ ì°¾ì„ ìˆ˜ ì—†ì„ ë•Œ ì‚¬ìš©)
        
        Returns:
            ì‹¤í–‰ ê²°ê³¼
        """
        try:
            # 1. í˜„ì¬ metadata ë¡œë“œ
            metadata_result = await self.simulation_service.load_scenario_metadata(scenario_id)
            metadata = metadata_result.get("metadata", {})
            
            if metadata is None:
                return {
                    "success": False,
                    "message": "ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "error": "Scenario not found",
                }
            
            process_flow = metadata.get("process_flow", [])
            
            if not process_flow:
                return {
                    "success": False,
                    "message": "ì‚­ì œí•  í”„ë¡œì„¸ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "error": "No processes to remove",
                }
            
            # 2. í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°
            normalized_name = normalize_process_name(process_name)
            process_index = None
            
            if step is not None:
                # step ë²ˆí˜¸ë¡œ ì°¾ê¸°
                for idx, p in enumerate(process_flow):
                    if p.get("step") == step:
                        process_index = idx
                        break
            else:
                # ì´ë¦„ìœ¼ë¡œ ì°¾ê¸°
                for idx, p in enumerate(process_flow):
                    if p.get("name") == normalized_name:
                        process_index = idx
                        break
            
            if process_index is None:
                return {
                    "success": False,
                    "message": f"í”„ë¡œì„¸ìŠ¤ '{process_name}'ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "error": f"Process '{normalized_name}' not found",
                }
            
            # 3. í”„ë¡œì„¸ìŠ¤ ì‚­ì œ
            removed_process = process_flow.pop(process_index)
            
            # 4. step ë²ˆí˜¸ ì¬ì •ë ¬
            for idx, p in enumerate(process_flow):
                p["step"] = idx
            
            metadata["process_flow"] = process_flow
            metadata["savedAt"] = datetime.now(timezone.utc).isoformat()
            
            # 5. S3ì— ì €ì¥
            await self.simulation_service.save_scenario_metadata(scenario_id, metadata)
            
            logger.info(f"âœ… Process '{normalized_name}' removed from scenario {scenario_id}")
            
            return {
                "success": True,
                "message": f"í”„ë¡œì„¸ìŠ¤ '{process_name}'ê°€ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
                "action": "remove_process",
                "data": {
                    "removed_process": removed_process,
                    "total_processes": len(process_flow),
                },
            }
        
        except Exception as e:
            logger.error(f"Failed to remove process: {str(e)}")
            return {
                "success": False,
                "message": f"í”„ë¡œì„¸ìŠ¤ ì‚­ì œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
            }
    
    async def list_files(self, scenario_id: str) -> Dict[str, Any]:
        """
        S3 í´ë”ì˜ íŒŒì¼ ëª©ë¡ ì¡°íšŒ
        
        Args:
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID
        
        Returns:
            íŒŒì¼ ëª©ë¡ ì •ë³´
        """
        try:
            files = await self.simulation_service.s3_manager.list_files_async(scenario_id)
            
            if not files:
                return {
                    "success": True,
                    "message": "S3 í´ë”ì— íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.",
                    "files": [],
                    "count": 0,
                }
            
            # íŒŒì¼ì„ ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ë¥˜
            file_categories = {
                "metadata": [f for f in files if "metadata" in f.lower()],
                "parquet": [f for f in files if f.endswith(".parquet")],
                "json": [f for f in files if f.endswith(".json") and "metadata" not in f.lower()],
                "other": [f for f in files if f not in [item for sublist in [
                    [f for f in files if "metadata" in f.lower()],
                    [f for f in files if f.endswith(".parquet")],
                    [f for f in files if f.endswith(".json") and "metadata" not in f.lower()]
                ] for item in sublist]]
            }
            
            message_parts = [f"ì´ {len(files)}ê°œì˜ íŒŒì¼ì´ ìˆìŠµë‹ˆë‹¤:\n"]
            
            if file_categories["metadata"]:
                message_parts.append(f"\nğŸ“„ ë©”íƒ€ë°ì´í„° íŒŒì¼ ({len(file_categories['metadata'])}ê°œ):")
                for f in file_categories["metadata"]:
                    message_parts.append(f"  - {f}")
            
            if file_categories["parquet"]:
                message_parts.append(f"\nğŸ“Š Parquet íŒŒì¼ ({len(file_categories['parquet'])}ê°œ):")
                for f in file_categories["parquet"]:
                    message_parts.append(f"  - {f}")
            
            if file_categories["json"]:
                message_parts.append(f"\nğŸ“‹ JSON íŒŒì¼ ({len(file_categories['json'])}ê°œ):")
                for f in file_categories["json"]:
                    message_parts.append(f"  - {f}")
            
            if file_categories["other"]:
                message_parts.append(f"\nğŸ“ ê¸°íƒ€ íŒŒì¼ ({len(file_categories['other'])}ê°œ):")
                for f in file_categories["other"]:
                    message_parts.append(f"  - {f}")
            
            return {
                "success": True,
                "message": "\n".join(message_parts),
                "files": files,
                "count": len(files),
                "categories": {k: len(v) for k, v in file_categories.items()},
            }
        
        except Exception as e:
            logger.error(f"Failed to list files: {str(e)}")
            return {
                "success": False,
                "message": f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
            }
    
    async def read_file(
        self,
        scenario_id: str,
        filename: str,
        summary_type: str = "summary"
    ) -> Dict[str, Any]:
        """
        S3 íŒŒì¼ ë‚´ìš© ì½ê¸° ë° ë¶„ì„
        
        Args:
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID
            filename: íŒŒì¼ ì´ë¦„
            summary_type: ìš”ì•½ íƒ€ì… (summary, full, structure)
        
        Returns:
            íŒŒì¼ ë‚´ìš© ë° ë¶„ì„ ê²°ê³¼
        """
        try:
            # 1. íŒŒì¼ ì½ê¸°
            if filename.endswith(".json"):
                # JSON íŒŒì¼ ì½ê¸°
                content = await self.simulation_service.s3_manager.get_json_async(
                    scenario_id=scenario_id,
                    filename=filename
                )
                
                if content is None:
                    return {
                        "success": False,
                        "message": f"íŒŒì¼ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                        "error": "File not found",
                    }
                
                # 2. ìš”ì•½ íƒ€ì…ì— ë”°ë¼ ì²˜ë¦¬
                if summary_type == "structure":
                    # êµ¬ì¡°ë§Œ ë°˜í™˜
                    import json
                    structure = self._get_json_structure(content)
                    return {
                        "success": True,
                        "message": f"íŒŒì¼ '{filename}'ì˜ êµ¬ì¡°:\n{json.dumps(structure, indent=2, ensure_ascii=False)}",
                        "filename": filename,
                        "structure": structure,
                    }
                elif summary_type == "full":
                    # ì „ì²´ ë‚´ìš© ë°˜í™˜ (í° íŒŒì¼ì€ ì£¼ì˜)
                    import json
                    return {
                        "success": True,
                        "message": f"íŒŒì¼ '{filename}'ì˜ ì „ì²´ ë‚´ìš©:\n{json.dumps(content, indent=2, ensure_ascii=False)[:5000]}...",
                        "filename": filename,
                        "content": content,
                    }
                else:
                    # summary: AIì—ê²Œ ì „ë‹¬í•˜ì—¬ ìš”ì•½
                    import json
                    
                    # íŒŒì¼ì´ í¬ë“  ì‘ë“  êµ¬ì¡°í™”ëœ ìš”ì•½ ì •ë³´ ì¶”ì¶œ
                    if isinstance(content, dict):
                        summary_info = {}
                        
                        # context ì •ë³´
                        if "context" in content:
                            ctx = content["context"]
                            summary_info["ì‹œë‚˜ë¦¬ì˜¤_ì •ë³´"] = {
                                "ê³µí•­": ctx.get("airport", ""),
                                "í„°ë¯¸ë„": ctx.get("terminal", ""),
                                "ë‚ ì§œ": ctx.get("date", ""),
                                "ì €ì¥_ì‹œê°": ctx.get("lastSavedAt", ""),
                            }
                        
                        # process_flow ì •ë³´ (êµ¬ì²´ì ìœ¼ë¡œ)
                        if "process_flow" in content:
                            pf = content["process_flow"]
                            summary_info["í”„ë¡œì„¸ìŠ¤_íë¦„"] = []
                            for proc in pf:  # ëª¨ë“  í”„ë¡œì„¸ìŠ¤
                                proc_info = {
                                    "ì´ë¦„": proc.get("name", ""),
                                    "ë‹¨ê³„": proc.get("step", ""),
                                    "ì´ë™ì‹œê°„_ë¶„": proc.get("travel_time_minutes", 0),
                                    "ê¸°ë³¸_ì²˜ë¦¬ì‹œê°„_ì´ˆ": proc.get("process_time_seconds", 0),
                                    "êµ¬ì—­_ê°œìˆ˜": len(proc.get("zones", {})),
                                }

                                # ê° êµ¬ì—­ì˜ ì‹œì„¤ ì •ë³´ (ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘: ìƒì„¸ ëª©ë¡ ëŒ€ì‹  ìš”ì•½ë§Œ)
                                zones_summary = {}
                                total_facilities = 0
                                active_facilities = 0
                                sample_operating_period = None
                                sample_process_time = None

                                for zone_name, zone_data in proc.get("zones", {}).items():
                                    facilities = zone_data.get("facilities", [])
                                    facility_count = len(facilities)
                                    total_facilities += facility_count

                                    zone_active = 0
                                    for fac in facilities:
                                        operating_schedule = fac.get("operating_schedule", {})
                                        time_blocks = operating_schedule.get("time_blocks", [])

                                        if time_blocks:
                                            tb = time_blocks[0]
                                            if tb.get("activate", False):
                                                zone_active += 1
                                                active_facilities += 1

                                            # ëŒ€í‘œ ê°’ ì €ì¥ (ì²« ë²ˆì§¸ í™œì„± ì‹œì„¤)
                                            if sample_operating_period is None and tb.get("activate", False):
                                                sample_operating_period = tb.get("period", "")
                                                sample_process_time = tb.get("process_time_seconds", proc.get("process_time_seconds", 0))

                                    zones_summary[zone_name] = {
                                        "ì‹œì„¤_ê°œìˆ˜": facility_count,
                                        "í™œì„±_ì‹œì„¤_ê°œìˆ˜": zone_active
                                    }

                                proc_info["êµ¬ì—­ë³„_ìš”ì•½"] = zones_summary
                                proc_info["ì´_ì‹œì„¤_ê°œìˆ˜"] = total_facilities
                                proc_info["í™œì„±_ì‹œì„¤_ê°œìˆ˜"] = active_facilities

                                # ëŒ€í‘œ ìš´ì˜ ì •ë³´
                                if sample_operating_period:
                                    proc_info["ìš´ì˜ê¸°ê°„_ì˜ˆì‹œ"] = sample_operating_period
                                if sample_process_time:
                                    proc_info["ì²˜ë¦¬ì‹œê°„_ì´ˆ_ì˜ˆì‹œ"] = sample_process_time

                                summary_info["í”„ë¡œì„¸ìŠ¤_íë¦„"].append(proc_info)
                        
                        # flight ì •ë³´ ìš”ì•½
                        if "flight" in content:
                            flight = content["flight"]
                            flight_summary = {}
                            if flight.get("selectedConditions"):
                                sc = flight["selectedConditions"]
                                flight_summary["í•„í„°_íƒ€ì…"] = sc.get("type", "")
                                flight_summary["ì¡°ê±´_ê°œìˆ˜"] = len(sc.get("conditions", []))
                                if sc.get("expected_flights"):
                                    flight_summary["ì„ íƒëœ_í•­ê³µí¸"] = sc["expected_flights"].get("selected", 0)
                                    flight_summary["ì „ì²´_í•­ê³µí¸"] = sc["expected_flights"].get("total", 0)
                            if flight.get("appliedFilterResult"):
                                afr = flight["appliedFilterResult"]
                                flight_summary["ì ìš©ëœ_í•„í„°_ê²°ê³¼"] = afr.get("total", 0)
                            if flight_summary:
                                summary_info["í•­ê³µí¸_ì •ë³´"] = flight_summary
                        
                        # passenger ì •ë³´ ìš”ì•½
                        if "passenger" in content:
                            passenger = content["passenger"]
                            pax_summary = {}
                            if passenger.get("settings"):
                                pax_summary["ìµœì†Œ_ë„ì°©_ì‹œê°„_ë¶„"] = passenger["settings"].get("min_arrival_minutes")
                            if passenger.get("pax_generation"):
                                pg = passenger["pax_generation"]
                                pax_summary["ìƒì„±_ê·œì¹™_ê°œìˆ˜"] = len(pg.get("rules", []))
                                if pg.get("default"):
                                    pax_summary["ê¸°ë³¸_ì ì¬ìœ¨"] = pg["default"].get("load_factor")
                            if passenger.get("pax_demographics"):
                                pd = passenger["pax_demographics"]
                                if pd.get("nationality"):
                                    nat = pd["nationality"]
                                    pax_summary["êµ­ì _ê·œì¹™_ê°œìˆ˜"] = len(nat.get("rules", []))
                                    pax_summary["ì‚¬ìš©ê°€ëŠ¥_êµ­ì _ìˆ˜"] = len(nat.get("available_values", []))
                                if pd.get("profile"):
                                    prof = pd["profile"]
                                    pax_summary["í”„ë¡œí•„_ê·œì¹™_ê°œìˆ˜"] = len(prof.get("rules", []))
                                    pax_summary["ì‚¬ìš©ê°€ëŠ¥_í”„ë¡œí•„_ìˆ˜"] = len(prof.get("available_values", []))
                            if pax_summary:
                                summary_info["ìŠ¹ê°_ì •ë³´"] = pax_summary
                        
                        # workflow ì •ë³´
                        if "workflow" in content:
                            wf = content["workflow"]
                            summary_info["ì‘ì—…_íë¦„"] = {
                                "í˜„ì¬_ë‹¨ê³„": wf.get("currentStep", ""),
                                "1ë‹¨ê³„_ì™„ë£Œ": wf.get("step1Completed", False),
                                "2ë‹¨ê³„_ì™„ë£Œ": wf.get("step2Completed", False),
                                "ì‚¬ìš©ê°€ëŠ¥_ë‹¨ê³„": wf.get("availableSteps", []),
                            }
                        
                        # savedAt ì •ë³´
                        if "savedAt" in content:
                            summary_info["ì €ì¥_ì‹œê°"] = content.get("savedAt", "")
                        
                        content_str = json.dumps(summary_info, indent=2, ensure_ascii=False)
                    else:
                        # dictê°€ ì•„ë‹Œ ê²½ìš°
                        content_str = json.dumps(content, indent=2, ensure_ascii=False)
                    
                    return {
                        "success": True,
                        "message": f"íŒŒì¼ '{filename}'ì˜ ë‚´ìš©ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.",
                        "filename": filename,
                        "content_preview": content_str[:60000],  # êµ¬ì¡°í™”ëœ ìš”ì•½ ì •ë³´ (ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘)
                        "full_content": content,  # ì „ì²´ ë‚´ìš©ì€ ë³„ë„ë¡œ ì „ë‹¬
                        "needs_ai_analysis": True,
                    }
            
            elif filename.endswith(".parquet"):
                # Parquet íŒŒì¼ ì½ê¸° ë° ë¶„ì„
                try:
                    df = await self.simulation_service.s3_manager.get_parquet_async(
                        scenario_id=scenario_id,
                        filename=filename
                    )

                    if df is None:
                        return {
                            "success": False,
                            "message": f"íŒŒì¼ '{filename}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                            "error": "File not found",
                        }

                    # íŒŒì¼ íƒ€ì…ë³„ë¡œ í•´ë‹¹ ì—”ë“œí¬ì¸íŠ¸ì˜ Response ë¡œì§ ì‚¬ìš©
                    if filename == "flight-schedule.parquet":
                        # FlightScheduleResponse ë¡œì§ ì‚¬ìš©
                        analysis = await self._analyze_flight_schedule_with_response(df, scenario_id)
                    elif filename == "show-up-passenger.parquet":
                        # ShowUpPassengerResponse ë¡œì§ ì‚¬ìš©
                        analysis = await self._analyze_show_up_passenger_with_response(df, scenario_id)
                    elif filename == "simulation-pax.parquet":
                        # Lambda ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ ê¸°ë°˜ ë¶„ì„
                        analysis = await self._analyze_simulation_pax_with_response(df, scenario_id)
                    else:
                        # ê¸°íƒ€ parquet: ê¸°ì¡´ ë¶„ì„ ë¡œì§ ì‚¬ìš©
                        analysis = await self._analyze_parquet(df, filename, scenario_id)

                    # summary_typeì— ë”°ë¼ ì²˜ë¦¬
                    if summary_type == "structure":
                        # êµ¬ì¡° ì •ë³´ë§Œ
                        return {
                            "success": True,
                            "message": f"íŒŒì¼ '{filename}'ì˜ êµ¬ì¡°:\n{analysis.get('structure_str', '')}",
                            "filename": filename,
                            "structure": analysis.get("structure"),
                        }
                    elif summary_type == "full":
                        # ì „ì²´ ë¶„ì„ ê²°ê³¼
                        return {
                            "success": True,
                            "message": f"íŒŒì¼ '{filename}'ì˜ ì „ì²´ ë¶„ì„:\n{analysis.get('full_summary', '')}",
                            "filename": filename,
                            "analysis": analysis,
                        }
                    else:
                        # summary: AIì—ê²Œ ë¶„ì„ ìš”ì²­
                        import json
                        content_str = json.dumps(analysis.get("summary_info", analysis), indent=2, ensure_ascii=False)

                        return {
                            "success": True,
                            "message": f"íŒŒì¼ '{filename}'ì˜ ë‚´ìš©ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤.",
                            "filename": filename,
                            "content_preview": content_str[:60000],  # ë³µì¡í•œ ì‹œë‚˜ë¦¬ì˜¤ ëŒ€ì‘
                            "full_content": analysis,
                            "needs_ai_analysis": True,
                        }

                except Exception as e:
                    logger.error(f"Failed to analyze parquet file: {str(e)}")
                    return {
                        "success": False,
                        "message": f"Parquet íŒŒì¼ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                        "error": str(e),
                    }
            
            else:
                return {
                    "success": False,
                    "message": f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ì…ë‹ˆë‹¤: {filename}",
                    "error": "Unsupported file type",
                }
        
        except Exception as e:
            logger.error(f"Failed to read file: {str(e)}")
            return {
                "success": False,
                "message": f"íŒŒì¼ ì½ê¸° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}",
                "error": str(e),
            }
    
    def _get_json_structure(self, obj: Any, max_depth: int = 3, current_depth: int = 0) -> Any:
        """JSON êµ¬ì¡° ì¶”ì¶œ (ì¬ê·€ì )"""
        if current_depth >= max_depth:
            return "..."

        if isinstance(obj, dict):
            return {k: self._get_json_structure(v, max_depth, current_depth + 1) for k, v in obj.items()}
        elif isinstance(obj, list):
            if len(obj) == 0:
                return []
            return [self._get_json_structure(obj[0], max_depth, current_depth + 1), f"... ({len(obj)} items)"]
        else:
            return type(obj).__name__

    async def _analyze_show_up_passenger(self, df, filename: str, scenario_id: str) -> Dict[str, Any]:
        """
        show-up-passenger.parquet ì „ìš© ë¶„ì„
        metadata.json ì„¤ì •ê³¼ ì‹¤ì œ ê²°ê³¼ë¥¼ ë¹„êµ ë¶„ì„

        Args:
            df: pandas DataFrame
            filename: íŒŒì¼ëª…
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import pandas as pd
        import numpy as np

        # 1. metadata.json ë¡œë“œ
        metadata = None
        try:
            metadata = await self.simulation_service.s3_manager.get_json_async(
                scenario_id=scenario_id,
                filename="metadata.json"
            )
        except Exception as e:
            logger.warning(f"Could not load metadata.json: {str(e)}")

        analysis = {
            "íŒŒì¼_ì •ë³´": {
                "íŒŒì¼ëª…": filename,
                "ì´_ìŠ¹ê°_ìˆ˜": len(df),
                "ì´_ì»¬ëŸ¼_ìˆ˜": len(df.columns),
                "ë©”ëª¨ë¦¬_ì‚¬ìš©ëŸ‰_MB": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            },
            "í•­ê³µí¸_í†µê³„": {},
            "í•­ê³µì‚¬ë³„_ë¶„ì„": {},
            "í„°ë¯¸ë„ë³„_ë¶„ì„": {},
            "ì¸êµ¬í†µê³„_ë¶„ì„": {},
            "ë„ì°©_íŒ¨í„´_ë¶„ì„": {},
            "ì„¤ì •_vs_ì‹¤ì œ": {},
            "sample_data": [],
        }

        # 2. í•­ê³µí¸ í†µê³„
        if "flight_number" in df.columns and "operating_carrier_iata" in df.columns:
            unique_flights = df[["operating_carrier_iata", "flight_number", "flight_date"]].drop_duplicates()
            analysis["í•­ê³µí¸_í†µê³„"]["ì´_í•­ê³µí¸_ìˆ˜"] = len(unique_flights)

            if "total_seats" in df.columns:
                unique_flights_with_seats = df[
                    ["operating_carrier_iata", "flight_number", "flight_date", "total_seats"]
                ].drop_duplicates(subset=["operating_carrier_iata", "flight_number", "flight_date"], keep="first")
                avg_seats = unique_flights_with_seats["total_seats"].mean()
                analysis["í•­ê³µí¸_í†µê³„"]["í‰ê· _ì¢Œì„_ìˆ˜"] = round(float(avg_seats), 2)

        # 3. í•­ê³µì‚¬ë³„ ë¶„ì„
        if "operating_carrier_name" in df.columns or "operating_carrier_iata" in df.columns:
            carrier_col = "operating_carrier_name" if "operating_carrier_name" in df.columns else "operating_carrier_iata"
            carrier_stats = df.groupby(carrier_col).agg({
                "flight_number": "count",  # ìŠ¹ê° ìˆ˜
            }).rename(columns={"flight_number": "ìŠ¹ê°_ìˆ˜"})

            # í•­ê³µí¸ ìˆ˜ ê³„ì‚°
            if "flight_number" in df.columns:
                flights_per_carrier = df.groupby(carrier_col)[
                    ["flight_number", "flight_date"]
                ].apply(lambda x: x.drop_duplicates().shape[0])
                carrier_stats["í•­ê³µí¸_ìˆ˜"] = flights_per_carrier

            # í‰ê·  íƒ‘ìŠ¹ë¥  ê³„ì‚° (ìŠ¹ê° ìˆ˜ / (í•­ê³µí¸ ìˆ˜ * í‰ê·  ì¢Œì„ ìˆ˜))
            if "total_seats" in df.columns and "í•­ê³µí¸_ìˆ˜" in carrier_stats.columns:
                avg_seats_per_carrier = df.groupby(carrier_col).agg({
                    "total_seats": lambda x: x.iloc[0] if len(x) > 0 else 0
                })["total_seats"]

                carrier_stats["í‰ê· _íƒ‘ìŠ¹ë¥ _%"] = (
                    carrier_stats["ìŠ¹ê°_ìˆ˜"] / (carrier_stats["í•­ê³µí¸_ìˆ˜"] * avg_seats_per_carrier) * 100
                ).round(2)

            # ìƒìœ„ 10ê°œ í•­ê³µì‚¬ë§Œ
            carrier_stats_sorted = carrier_stats.sort_values("ìŠ¹ê°_ìˆ˜", ascending=False).head(10)
            analysis["í•­ê³µì‚¬ë³„_ë¶„ì„"] = carrier_stats_sorted.to_dict("index")

        # 4. í„°ë¯¸ë„ë³„ ë¶„ì„
        for terminal_col in ["departure_terminal", "arrival_terminal"]:
            if terminal_col in df.columns:
                terminal_stats = df[terminal_col].value_counts().head(10).to_dict()
                analysis["í„°ë¯¸ë„ë³„_ë¶„ì„"][terminal_col] = {
                    str(k): int(v) for k, v in terminal_stats.items()
                }

        # 5. ì¸êµ¬í†µê³„ ë¶„ì„ (nationality, profile)
        for demo_col in ["nationality", "profile"]:
            if demo_col in df.columns:
                # ì‹¤ì œ ë¶„í¬
                actual_dist = df[demo_col].value_counts()
                actual_pct = (actual_dist / len(df) * 100).round(2)

                demo_analysis = {
                    "ì‹¤ì œ_ë¶„í¬_ìŠ¹ê°ìˆ˜": actual_dist.head(10).to_dict(),
                    "ì‹¤ì œ_ë¶„í¬_%": actual_pct.head(10).to_dict(),
                }

                # metadata ì„¤ì •ê³¼ ë¹„êµ
                if metadata and "passenger" in metadata:
                    pax_demographics = metadata["passenger"].get("pax_demographics", {})
                    if demo_col in pax_demographics:
                        config = pax_demographics[demo_col]
                        default_dist = config.get("default", {})
                        # flightCount ì œì™¸
                        default_dist = {k: v for k, v in default_dist.items() if k != "flightCount"}
                        if default_dist:
                            demo_analysis["ì„¤ì •ëœ_ë¶„í¬_%"] = default_dist

                            # ì°¨ì´ ê³„ì‚°
                            diff = {}
                            for key in default_dist.keys():
                                actual_val = actual_pct.get(key, 0)
                                config_val = default_dist[key]
                                diff[key] = round(actual_val - config_val, 2)
                            demo_analysis["ì°¨ì´_%_ì‹¤ì œë¹¼ê¸°ì„¤ì •"] = diff

                analysis["ì¸êµ¬í†µê³„_ë¶„ì„"][demo_col] = demo_analysis

        # 6. ë„ì°© íŒ¨í„´ ë¶„ì„
        if "show_up_time" in df.columns and "scheduled_departure_local" in df.columns:
            # ì¶œë°œì‹œê°„ ëŒ€ë¹„ ë„ì°©ì‹œê°„ ì°¨ì´ (ë¶„ ë‹¨ìœ„)
            df_temp = df.copy()
            df_temp["show_up_time"] = pd.to_datetime(df_temp["show_up_time"])
            df_temp["scheduled_departure_local"] = pd.to_datetime(df_temp["scheduled_departure_local"])
            df_temp["minutes_before"] = (
                df_temp["scheduled_departure_local"] - df_temp["show_up_time"]
            ).dt.total_seconds() / 60

            arrival_stats = {
                "í‰ê· _ë„ì°©ì‹œê°„_ë¶„ì „": round(float(df_temp["minutes_before"].mean()), 2),
                "ì¤‘ì•™ê°’_ë„ì°©ì‹œê°„_ë¶„ì „": round(float(df_temp["minutes_before"].median()), 2),
                "í‘œì¤€í¸ì°¨_ë¶„": round(float(df_temp["minutes_before"].std()), 2),
                "ìµœì†Œ_ë„ì°©ì‹œê°„_ë¶„ì „": round(float(df_temp["minutes_before"].min()), 2),
                "ìµœëŒ€_ë„ì°©ì‹œê°„_ë¶„ì „": round(float(df_temp["minutes_before"].max()), 2),
            }

            # metadata ì„¤ì •ê³¼ ë¹„êµ
            if metadata and "passenger" in metadata:
                pax_arrival = metadata["passenger"].get("pax_arrival_patterns", {})
                default_pattern = pax_arrival.get("default", {})
                if default_pattern:
                    arrival_stats["ì„¤ì •ëœ_í‰ê· _ë¶„"] = default_pattern.get("mean")
                    arrival_stats["ì„¤ì •ëœ_í‘œì¤€í¸ì°¨_ë¶„"] = default_pattern.get("std")

                    # ì°¨ì´
                    if "mean" in default_pattern:
                        arrival_stats["í‰ê· _ì°¨ì´_ë¶„_ì‹¤ì œë¹¼ê¸°ì„¤ì •"] = round(
                            arrival_stats["í‰ê· _ë„ì°©ì‹œê°„_ë¶„ì „"] - default_pattern["mean"], 2
                        )

                settings = metadata["passenger"].get("settings", {})
                if "min_arrival_minutes" in settings:
                    arrival_stats["ì„¤ì •ëœ_ìµœì†Œë„ì°©ì‹œê°„_ë¶„"] = settings["min_arrival_minutes"]

            analysis["ë„ì°©_íŒ¨í„´_ë¶„ì„"] = arrival_stats

            # ì‹œê°„ëŒ€ë³„ ë¶„í¬ (ìƒìœ„ 20ê°œ)
            df_temp["show_up_hour"] = df_temp["show_up_time"].dt.strftime("%Y-%m-%d %H:00")
            hourly_dist = df_temp["show_up_hour"].value_counts().sort_index().head(20)
            analysis["ë„ì°©_íŒ¨í„´_ë¶„ì„"]["ì‹œê°„ëŒ€ë³„_ìŠ¹ê°ìˆ˜_ìƒìœ„20"] = hourly_dist.to_dict()

        # 7. íƒ‘ìŠ¹ë¥  ë¶„ì„ (ì„¤ì • vs ì‹¤ì œ)
        if metadata and "passenger" in metadata:
            pax_generation = metadata["passenger"].get("pax_generation", {})
            default_load_factor = pax_generation.get("default", {}).get("load_factor")

            if default_load_factor is not None:
                # ì„¤ì •ê°’
                display_lf = int(default_load_factor) if default_load_factor > 1 else int(default_load_factor * 100)
                analysis["ì„¤ì •_vs_ì‹¤ì œ"]["ì„¤ì •ëœ_íƒ‘ìŠ¹ë¥ _%"] = display_lf

                # ì‹¤ì œ íƒ‘ìŠ¹ë¥  ê³„ì‚°
                if "total_seats" in df.columns and "í•­ê³µí¸_ìˆ˜" in analysis["í•­ê³µí¸_í†µê³„"]:
                    total_passengers = len(df)
                    total_flights = analysis["í•­ê³µí¸_í†µê³„"]["ì´_í•­ê³µí¸_ìˆ˜"]
                    avg_seats = analysis["í•­ê³µí¸_í†µê³„"].get("í‰ê· _ì¢Œì„_ìˆ˜", 0)

                    if total_flights > 0 and avg_seats > 0:
                        actual_lf = (total_passengers / (total_flights * avg_seats)) * 100
                        analysis["ì„¤ì •_vs_ì‹¤ì œ"]["ì‹¤ì œ_íƒ‘ìŠ¹ë¥ _%"] = round(actual_lf, 2)
                        analysis["ì„¤ì •_vs_ì‹¤ì œ"]["íƒ‘ìŠ¹ë¥ _ì°¨ì´_%"] = round(actual_lf - display_lf, 2)

        # 8. ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 3ê°œ)
        sample_rows = df.head(3).to_dict("records")
        for row in sample_rows:
            clean_row = {}
            for k, v in row.items():
                if pd.isna(v):
                    clean_row[k] = None
                elif isinstance(v, (pd.Timestamp, np.datetime64)):
                    clean_row[k] = str(v)
                elif isinstance(v, (pd.Timedelta, np.timedelta64)):
                    clean_row[k] = str(v)
                elif isinstance(v, (np.integer, np.floating)):
                    clean_row[k] = float(v) if isinstance(v, np.floating) else int(v)
                else:
                    clean_row[k] = str(v) if not isinstance(v, (str, int, float, bool)) else v
            analysis["sample_data"].append(clean_row)

        # ìš”ì•½ ì •ë³´ ìƒì„±
        summary_info = {
            "íŒŒì¼ëª…": filename,
            "íŒŒì¼_ì •ë³´": analysis["íŒŒì¼_ì •ë³´"],
            "í•­ê³µí¸_í†µê³„": analysis["í•­ê³µí¸_í†µê³„"],
            "í•­ê³µì‚¬ë³„_ë¶„ì„_ìƒìœ„10": analysis["í•­ê³µì‚¬ë³„_ë¶„ì„"],
            "ì¸êµ¬í†µê³„_ë¶„ì„": analysis["ì¸êµ¬í†µê³„_ë¶„ì„"],
            "ë„ì°©_íŒ¨í„´_ë¶„ì„": analysis["ë„ì°©_íŒ¨í„´_ë¶„ì„"],
            "ì„¤ì •_vs_ì‹¤ì œ": analysis["ì„¤ì •_vs_ì‹¤ì œ"],
        }

        # êµ¬ì¡° ë¬¸ìì—´
        structure_str = f"ì´ {len(df):,}ëª… ìŠ¹ê°, {len(df.columns)}ê°œ ì»¬ëŸ¼\n"
        structure_str += f"í•­ê³µí¸ ìˆ˜: {analysis['í•­ê³µí¸_í†µê³„'].get('ì´_í•­ê³µí¸_ìˆ˜', 0)}í¸\n"
        structure_str += f"ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns.tolist()[:15])}"
        if len(df.columns) > 15:
            structure_str += f"... (ì™¸ {len(df.columns) - 15}ê°œ)"

        # ì „ì²´ ìš”ì•½
        full_summary = f"íŒŒì¼: {filename}\n"
        full_summary += f"ì´ ìŠ¹ê° ìˆ˜: {len(df):,}ëª…\n"
        full_summary += f"ì´ í•­ê³µí¸ ìˆ˜: {analysis['í•­ê³µí¸_í†µê³„'].get('ì´_í•­ê³µí¸_ìˆ˜', 0)}í¸\n"
        full_summary += f"í‰ê·  ì¢Œì„ ìˆ˜: {analysis['í•­ê³µí¸_í†µê³„'].get('í‰ê· _ì¢Œì„_ìˆ˜', 0)}\n"
        if "ì„¤ì •_vs_ì‹¤ì œ" in analysis and "ì‹¤ì œ_íƒ‘ìŠ¹ë¥ _%" in analysis["ì„¤ì •_vs_ì‹¤ì œ"]:
            full_summary += f"ì‹¤ì œ íƒ‘ìŠ¹ë¥ : {analysis['ì„¤ì •_vs_ì‹¤ì œ']['ì‹¤ì œ_íƒ‘ìŠ¹ë¥ _%']}%\n"

        analysis["summary_info"] = summary_info
        analysis["structure"] = {"rows": len(df), "columns": df.columns.tolist()}
        analysis["structure_str"] = structure_str
        analysis["full_summary"] = full_summary

        return analysis

    async def _analyze_parquet(self, df, filename: str, scenario_id: str = None) -> Dict[str, Any]:
        """
        Parquet íŒŒì¼ ë¶„ì„ (íŒŒì¼ íƒ€ì…ë³„ ìµœì í™”)

        Args:
            df: pandas DataFrame
            filename: íŒŒì¼ëª…
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID (show-up-passenger ë¶„ì„ ì‹œ metadata ë¡œë“œìš©)

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        import pandas as pd
        import numpy as np

        # show-up-passenger.parquet íŠ¹í™” ë¶„ì„
        if "show-up-passenger" in filename and scenario_id:
            return await self._analyze_show_up_passenger(df, filename, scenario_id)

        # simulation-pax.parquet ë˜ëŠ” ì¼ë°˜ parquet ë¶„ì„
        analysis = {
            "basic_info": {
                "ì´_í–‰_ìˆ˜": len(df),
                "ì´_ì»¬ëŸ¼_ìˆ˜": len(df.columns),
                "ë©”ëª¨ë¦¬_ì‚¬ìš©ëŸ‰_MB": round(df.memory_usage(deep=True).sum() / 1024 / 1024, 2),
            },
            "columns": {},
            "process_analysis": {},
            "sample_data": [],
        }

        # ì»¬ëŸ¼ ì •ë³´
        for col in df.columns:
            dtype_str = str(df[col].dtype)
            null_count = int(df[col].isnull().sum())
            null_pct = round(null_count / len(df) * 100, 2) if len(df) > 0 else 0

            col_info = {
                "ë°ì´í„°_íƒ€ì…": dtype_str,
                "ê²°ì¸¡ê°’_ìˆ˜": null_count,
                "ê²°ì¸¡ê°’_ë¹„ìœ¨": f"{null_pct}%",
            }

            # ìˆ«ìí˜• ì»¬ëŸ¼ í†µê³„
            if pd.api.types.is_numeric_dtype(df[col]):
                col_info["í†µê³„"] = {
                    "í‰ê· ": round(float(df[col].mean()), 2) if not df[col].isnull().all() else None,
                    "ìµœì†Œ": round(float(df[col].min()), 2) if not df[col].isnull().all() else None,
                    "ìµœëŒ€": round(float(df[col].max()), 2) if not df[col].isnull().all() else None,
                }
            # ë²”ì£¼í˜• ì»¬ëŸ¼ í†µê³„
            elif pd.api.types.is_object_dtype(df[col]) or pd.api.types.is_categorical_dtype(df[col]):
                unique_count = df[col].nunique()
                col_info["ê³ ìœ ê°’_ìˆ˜"] = unique_count
                if unique_count <= 20:  # ê³ ìœ ê°’ì´ ì ìœ¼ë©´ ê°’ ë¶„í¬ í‘œì‹œ
                    value_counts = df[col].value_counts().head(10).to_dict()
                    col_info["ê°’_ë¶„í¬"] = {str(k): int(v) for k, v in value_counts.items()}

            analysis["columns"][col] = col_info

        # simulation-pax.parquet íŠ¹í™” ë¶„ì„
        # í”„ë¡œì„¸ìŠ¤ë³„ ë¶„ì„ (ì»¬ëŸ¼ëª… íŒ¨í„´: {process_name}_status, {process_name}_done_time ë“±)
        process_names = set()
        for col in df.columns:
            if "_status" in col:
                process_name = col.replace("_status", "")
                process_names.add(process_name)

        for process_name in sorted(process_names):
            process_info = {}

            # Status ë¶„ì„
            status_col = f"{process_name}_status"
            if status_col in df.columns:
                status_counts = df[status_col].value_counts().to_dict()
                process_info["status_ë¶„í¬"] = {str(k): int(v) for k, v in status_counts.items()}

                total_passengers = len(df)
                if "completed" in status_counts:
                    process_info["ì™„ë£Œìœ¨"] = f"{round(status_counts['completed'] / total_passengers * 100, 2)}%"

            # ëŒ€ê¸° ì‹œê°„ ë¶„ì„
            queue_wait_col = f"{process_name}_queue_wait_time"
            if queue_wait_col in df.columns:
                # Timedeltaë¥¼ ì´ˆ ë‹¨ìœ„ë¡œ ë³€í™˜
                wait_times = df[queue_wait_col].dropna()
                if len(wait_times) > 0:
                    # Timedeltaë¥¼ ì´ˆë¡œ ë³€í™˜
                    if pd.api.types.is_timedelta64_dtype(wait_times):
                        wait_seconds = wait_times.dt.total_seconds()
                        process_info["ëŒ€ê¸°ì‹œê°„_í†µê³„_ì´ˆ"] = {
                            "í‰ê· ": round(float(wait_seconds.mean()), 2),
                            "ì¤‘ì•™ê°’": round(float(wait_seconds.median()), 2),
                            "ìµœì†Œ": round(float(wait_seconds.min()), 2),
                            "ìµœëŒ€": round(float(wait_seconds.max()), 2),
                        }

            # í ê¸¸ì´ ë¶„ì„
            queue_length_col = f"{process_name}_queue_length"
            if queue_length_col in df.columns:
                queue_lengths = df[queue_length_col].dropna()
                if len(queue_lengths) > 0:
                    process_info["í_ê¸¸ì´_í†µê³„"] = {
                        "í‰ê· ": round(float(queue_lengths.mean()), 2),
                        "ìµœëŒ€": round(float(queue_lengths.max()), 2),
                    }

            # ì‹œì„¤ ì‚¬ìš© ë¶„ì„
            facility_col = f"{process_name}_facility"
            if facility_col in df.columns:
                facility_counts = df[facility_col].value_counts().head(10).to_dict()
                process_info["ìƒìœ„_ì‹œì„¤_ì‚¬ìš©"] = {str(k): int(v) for k, v in facility_counts.items()}

            if process_info:
                analysis["process_analysis"][process_name] = process_info

        # ìƒ˜í”Œ ë°ì´í„° (ì²˜ìŒ 5ê°œ í–‰)
        sample_rows = df.head(5).to_dict('records')
        for row in sample_rows:
            # Timestamp, Timedelta ë“±ì„ ë¬¸ìì—´ë¡œ ë³€í™˜
            clean_row = {}
            for k, v in row.items():
                if pd.isna(v):
                    clean_row[k] = None
                elif isinstance(v, (pd.Timestamp, np.datetime64)):
                    clean_row[k] = str(v)
                elif isinstance(v, (pd.Timedelta, np.timedelta64)):
                    clean_row[k] = str(v)
                elif isinstance(v, (np.integer, np.floating)):
                    clean_row[k] = float(v) if isinstance(v, np.floating) else int(v)
                else:
                    clean_row[k] = str(v) if not isinstance(v, (str, int, float, bool)) else v
            analysis["sample_data"].append(clean_row)

        # ìš”ì•½ ì •ë³´ ìƒì„±
        summary_info = {
            "íŒŒì¼ëª…": filename,
            "ê¸°ë³¸_ì •ë³´": analysis["basic_info"],
            "í”„ë¡œì„¸ìŠ¤_ë¶„ì„": analysis["process_analysis"],
        }

        # êµ¬ì¡° ë¬¸ìì—´
        structure_str = f"ì´ {len(df):,}í–‰, {len(df.columns)}ê°œ ì»¬ëŸ¼\n"
        structure_str += f"ì»¬ëŸ¼ ëª©ë¡: {', '.join(df.columns.tolist()[:20])}"
        if len(df.columns) > 20:
            structure_str += f"... (ì™¸ {len(df.columns) - 20}ê°œ)"

        # ì „ì²´ ìš”ì•½
        full_summary = f"íŒŒì¼: {filename}\n"
        full_summary += f"ì´ ìŠ¹ê° ìˆ˜: {len(df):,}ëª…\n"
        full_summary += f"ì´ ì»¬ëŸ¼ ìˆ˜: {len(df.columns)}ê°œ\n"
        full_summary += f"í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(process_names)}ê°œ\n"
        if process_names:
            full_summary += f"í”„ë¡œì„¸ìŠ¤ ëª©ë¡: {', '.join(sorted(process_names))}\n"

        analysis["summary_info"] = summary_info
        analysis["structure"] = {"rows": len(df), "columns": df.columns.tolist()}
        analysis["structure_str"] = structure_str
        analysis["full_summary"] = full_summary

        return analysis

    async def _analyze_flight_schedule_with_response(self, df, scenario_id: str) -> Dict[str, Any]:
        """
        flight-schedule.parquet ë¶„ì„ - FlightScheduleResponse ë¡œì§ ì‚¬ìš©

        Args:
            df: pandas DataFrame
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID

        Returns:
            FlightScheduleResponse.build_response() ê²°ê³¼
        """
        from app.routes.simulation.application.core.flight_schedules import FlightScheduleResponse

        # metadataì—ì„œ airport, date, flight_type ì¶”ì¶œ
        metadata = None
        try:
            metadata = await self.simulation_service.s3_manager.get_json_async(
                scenario_id=scenario_id,
                filename="metadata.json"
            )
        except Exception as e:
            logger.warning(f"Could not load metadata.json: {str(e)}")

        # metadataì—ì„œ ì •ë³´ ì¶”ì¶œ
        airport = None
        date = None
        if metadata and "context" in metadata:
            airport = metadata["context"].get("airport")
            date = metadata["context"].get("date")

        # flight_typeì€ ê¸°ë³¸ê°’ "departure" ì‚¬ìš© (metadataì—ëŠ” ì—†ìŒ)
        flight_type = "departure"

        # DataFrameì„ list of dictë¡œ ë³€í™˜
        flight_schedule_data = df.to_dict("records")

        # FlightScheduleResponseë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response_builder = FlightScheduleResponse()
        response_data = await response_builder.build_response(
            flight_schedule_data=flight_schedule_data,
            applied_conditions=None,
            flight_type=flight_type,
            airport=airport,
            date=date,
            scenario_id=scenario_id
        )

        # í•­ê³µê¸° ì •ë³´ ì¶”ì¶œ
        aircraft_info = self._extract_aircraft_info(df)

        # ëª©ì ì§€ë³„ í•­ê³µí¸ ë¶„ì„ (flight-scheduleë„ ë™ì¼í•œ ì»¬ëŸ¼ êµ¬ì¡°)
        destination_analysis = self._analyze_destinations(df)

        # AIì—ê²Œ ì „ë‹¬í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        summary_info = {
            "íŒŒì¼ëª…": "flight-schedule.parquet",
            "ê¸°ë³¸_ì •ë³´": {
                "ì´_í•­ê³µí¸_ìˆ˜": response_data.get("total", 0),
                "ê³µí•­": response_data.get("airport"),
                "ë‚ ì§œ": response_data.get("date"),
            },
            "ëª©ì ì§€ë³„_í•­ê³µí¸": destination_analysis,
            "í•­ê³µê¸°_ì •ë³´": aircraft_info,
            "ì°¨íŠ¸_ë°ì´í„°": response_data.get("chart_y_data", {}),
            "parquet_ë©”íƒ€ë°ì´í„°": response_data.get("parquet_metadata", [])
        }

        structure_str = f"ì´ {response_data.get('total', 0):,}í¸, {len(df.columns)}ê°œ ì»¬ëŸ¼"
        full_summary = f"íŒŒì¼: flight-schedule.parquet\n"
        full_summary += f"ì´ í•­ê³µí¸ ìˆ˜: {response_data.get('total', 0):,}í¸\n"

        return {
            "summary_info": summary_info,
            "structure": {"rows": len(df), "columns": df.columns.tolist()},
            "structure_str": structure_str,
            "full_summary": full_summary,
            "response_data": response_data,  # ì „ì²´ ì‘ë‹µ ë°ì´í„°ë„ í¬í•¨
        }

    async def _analyze_show_up_passenger_with_response(self, df, scenario_id: str) -> Dict[str, Any]:
        """
        show-up-passenger.parquet ë¶„ì„ - ShowUpPassengerResponse ë¡œì§ ì‚¬ìš©

        Args:
            df: pandas DataFrame
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID

        Returns:
            ShowUpPassengerResponse.build_response() ê²°ê³¼
        """
        from app.routes.simulation.application.core.show_up_pax import ShowUpPassengerResponse

        # metadata ë¡œë“œ (configë¡œ ì‚¬ìš©)
        metadata = None
        try:
            metadata = await self.simulation_service.s3_manager.get_json_async(
                scenario_id=scenario_id,
                filename="metadata.json"
            )
        except Exception as e:
            logger.warning(f"Could not load metadata.json: {str(e)}")

        # ëª©ì ì§€ë³„ í•­ê³µí¸ ë¶„ì„ (metadata ì—†ì´ë„ ì‹¤í–‰ ê°€ëŠ¥)
        destination_analysis = self._analyze_destinations(df)

        # í•­ê³µê¸° ì •ë³´ ì¶”ì¶œ
        aircraft_info = self._extract_aircraft_info(df)

        # ìŠ¹ê° ë„ì°© ì‹œê°„ í†µê³„ ì¶”ì¶œ
        passenger_arrival_stats = self._extract_passenger_arrival_stats(df)

        # êµ­ì  ë° í”„ë¡œí•„ ì •ë³´ ì¶”ì¶œ
        demographics_info = self._extract_demographics_info(df)

        if metadata is None:
            # metadataê°€ ì—†ëŠ” ê²½ìš° ê°„ë‹¨í•œ ì‘ë‹µ ìƒì„±
            airport_val = df['departure_airport_iata'].iloc[0] if 'departure_airport_iata' in df.columns and len(df) > 0 else None
            date_val = df['flight_date'].iloc[0] if 'flight_date' in df.columns and len(df) > 0 else None

            # Timestampë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
            if date_val is not None:
                date_val = str(date_val)

            summary_info = {
                "íŒŒì¼ëª…": "show-up-passenger.parquet",
                "ê¸°ë³¸_ì •ë³´": {
                    "ì´_ìŠ¹ê°_ìˆ˜": len(df),
                    "ê³µí•­": str(airport_val) if airport_val is not None else None,
                    "ë‚ ì§œ": date_val,
                },
                "ëª©ì ì§€ë³„_í•­ê³µí¸": destination_analysis,
                "í•­ê³µê¸°_ì •ë³´": aircraft_info,
                "ìŠ¹ê°_ë„ì°©_í†µê³„": passenger_arrival_stats,
                "ìŠ¹ê°_ì¸êµ¬í†µê³„": demographics_info,
            }

            return {
                "summary_info": summary_info,
                "structure": {"rows": len(df), "columns": df.columns.tolist()},
                "structure_str": f"ì´ {len(df):,}ëª… ìŠ¹ê°, {len(df.columns)}ê°œ ì»¬ëŸ¼",
                "full_summary": f"íŒŒì¼: show-up-passenger.parquet\nì´ ìŠ¹ê° ìˆ˜: {len(df):,}ëª…\n",
            }

        # metadataì—ì„œ ì •ë³´ ì¶”ì¶œ
        airport = None
        date = None
        if "context" in metadata:
            airport = metadata["context"].get("airport")
            date = metadata["context"].get("date")

        # config êµ¬ì„± (passenger ì„¤ì • ì‚¬ìš©)
        config = {
            "settings": metadata.get("passenger", {}).get("settings", {}),
            "pax_generation": metadata.get("passenger", {}).get("pax_generation", {}),
            "pax_demographics": metadata.get("passenger", {}).get("pax_demographics", {}),
            "pax_arrival_patterns": metadata.get("passenger", {}).get("pax_arrival_patterns", {}),
        }

        # ShowUpPassengerResponseë¥¼ ì‚¬ìš©í•˜ì—¬ ì‘ë‹µ ìƒì„±
        response_builder = ShowUpPassengerResponse()
        response_data = await response_builder.build_response(
            pax_df=df,
            config=config,
            airport=airport,
            date=date,
            scenario_id=scenario_id
        )

        # AIì—ê²Œ ì „ë‹¬í•  í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        summary_info = {
            "íŒŒì¼ëª…": "show-up-passenger.parquet",
            "ê¸°ë³¸_ì •ë³´": {
                "ì´_ìŠ¹ê°_ìˆ˜": response_data.get("total", 0),
                "ê³µí•­": response_data.get("airport"),
                "ë‚ ì§œ": response_data.get("date"),
            },
            "ìš”ì•½_ì •ë³´": response_data.get("summary", {}),
            "ëª©ì ì§€ë³„_í•­ê³µí¸": destination_analysis,  # ëª©ì ì§€ë³„ ë¶„ì„ ì¶”ê°€
            "í•­ê³µê¸°_ì •ë³´": aircraft_info,
            "ìŠ¹ê°_ë„ì°©_í†µê³„": passenger_arrival_stats,
            "ìŠ¹ê°_ì¸êµ¬í†µê³„": demographics_info,  # êµ­ì /í”„ë¡œí•„ ì •ë³´ ì¶”ê°€
            "ì°¨íŠ¸_ë°ì´í„°": response_data.get("chart_y_data", {}),
        }

        structure_str = f"ì´ {response_data.get('total', 0):,}ëª… ìŠ¹ê°, {len(df.columns)}ê°œ ì»¬ëŸ¼"
        full_summary = f"íŒŒì¼: show-up-passenger.parquet\n"
        full_summary += f"ì´ ìŠ¹ê° ìˆ˜: {response_data.get('total', 0):,}ëª…\n"

        summary = response_data.get("summary", {})
        if summary:
            full_summary += f"ì´ í•­ê³µí¸ ìˆ˜: {summary.get('flights', 0)}í¸\n"
            full_summary += f"í‰ê·  ì¢Œì„ ìˆ˜: {summary.get('avg_seats', 0)}\n"
            full_summary += f"íƒ‘ìŠ¹ë¥ : {summary.get('load_factor', 0)}%\n"

        return {
            "summary_info": summary_info,
            "structure": {"rows": len(df), "columns": df.columns.tolist()},
            "structure_str": structure_str,
            "full_summary": full_summary,
            "response_data": response_data,  # ì „ì²´ ì‘ë‹µ ë°ì´í„°ë„ í¬í•¨
        }

    def _extract_aircraft_info(self, df) -> Dict[str, Any]:
        """
        í•­ê³µê¸° ì •ë³´ ì¶”ì¶œ

        Args:
            df: pandas DataFrame

        Returns:
            í•­ê³µê¸° ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        import pandas as pd

        aircraft_info = {}

        # aircraft_type_iata ì •ë³´ ì¶”ì¶œ
        if 'aircraft_type_iata' in df.columns:
            aircraft_types = df['aircraft_type_iata'].dropna().unique()
            if len(aircraft_types) > 0:
                # ê¸°ì¢… ì½”ë“œ ë§¤í•‘ (ì¼ë°˜ì ì¸ IATA ì½”ë“œ)
                aircraft_name_map = {
                    '738': 'Boeing 737-800',
                    '73H': 'Boeing 737-800',
                    '320': 'Airbus A320',
                    '321': 'Airbus A321',
                    '359': 'Airbus A350-900',
                    '77W': 'Boeing 777-300ER',
                    '788': 'Boeing 787-8',
                    '789': 'Boeing 787-9',
                }

                aircraft_list = []
                for code in aircraft_types:
                    code_str = str(code)
                    full_name = aircraft_name_map.get(code_str, code_str)
                    aircraft_list.append({
                        "ê¸°ì¢…_ì½”ë“œ": code_str,
                        "ê¸°ì¢…_ëª…": full_name
                    })

                aircraft_info["ì‚¬ìš©_ê¸°ì¢…"] = aircraft_list
                if len(aircraft_list) == 1:
                    aircraft_info["ì„¤ëª…"] = f"ëª¨ë“  í•­ê³µí¸ì´ {aircraft_list[0]['ê¸°ì¢…_ëª…']}ì„ ì‚¬ìš©í•©ë‹ˆë‹¤"
                else:
                    aircraft_info["ì„¤ëª…"] = f"ì´ {len(aircraft_list)}ê°œ ê¸°ì¢…ì„ ì‚¬ìš©í•©ë‹ˆë‹¤"

        # total_seats ì •ë³´ë„ ì¶”ê°€
        if 'total_seats' in df.columns:
            seats = df['total_seats'].dropna()
            if len(seats) > 0:
                unique_seats = seats.unique()
                if len(unique_seats) == 1:
                    aircraft_info["ì¢Œì„_ìˆ˜"] = f"{int(unique_seats[0])}ì„"
                else:
                    aircraft_info["ì¢Œì„_ìˆ˜_ë²”ìœ„"] = f"{int(seats.min())}~{int(seats.max())}ì„"

        return aircraft_info if aircraft_info else {"ì„¤ëª…": "í•­ê³µê¸° ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"}

    def _extract_demographics_info(self, df) -> Dict[str, Any]:
        """ìŠ¹ê° êµ­ì  ë° í”„ë¡œí•„ ì •ë³´ ì¶”ì¶œ"""
        import pandas as pd

        demographics = {}

        # êµ­ì  ì •ë³´
        if 'nationality' in df.columns:
            nationalities = df['nationality'].dropna()
            if len(nationalities) > 0:
                nat_counts = nationalities.value_counts()
                demographics["êµ­ì _ë¶„í¬"] = {
                    str(nat): int(count) for nat, count in nat_counts.items()
                }
                demographics["ì´_êµ­ì _ìˆ˜"] = len(nat_counts)

        # í”„ë¡œí•„ ì •ë³´
        if 'profile' in df.columns:
            profiles = df['profile'].dropna()
            if len(profiles) > 0:
                prof_counts = profiles.value_counts()
                demographics["í”„ë¡œí•„_ë¶„í¬"] = {
                    str(prof): int(count) for prof, count in prof_counts.items()
                }
                demographics["ì´_í”„ë¡œí•„_ìˆ˜"] = len(prof_counts)

        return demographics if demographics else {"ì„¤ëª…": "êµ­ì  ë° í”„ë¡œí•„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"}

    def _extract_passenger_arrival_stats(self, df) -> Dict[str, Any]:
        """
        ìŠ¹ê° ë„ì°© ì‹œê°„ í†µê³„ ì¶”ì¶œ

        Args:
            df: pandas DataFrame

        Returns:
            ìŠ¹ê° ë„ì°© ì‹œê°„ í†µê³„
        """
        import pandas as pd
        import numpy as np

        stats = {}

        # show_up_timeê³¼ scheduled_departure_local ë¹„êµ
        if 'show_up_time' in df.columns and 'scheduled_departure_local' in df.columns:
            df_temp = df.copy()
            df_temp['show_up_time'] = pd.to_datetime(df_temp['show_up_time'])
            df_temp['scheduled_departure_local'] = pd.to_datetime(df_temp['scheduled_departure_local'])

            # ì¶œë°œ ì „ ë„ì°© ì‹œê°„ ê³„ì‚° (ë¶„ ë‹¨ìœ„)
            valid_mask = df_temp['show_up_time'].notna() & df_temp['scheduled_departure_local'].notna()
            if valid_mask.sum() > 0:
                time_before_departure = (
                    df_temp.loc[valid_mask, 'scheduled_departure_local'] -
                    df_temp.loc[valid_mask, 'show_up_time']
                ).dt.total_seconds() / 60

                stats["í‰ê· _ë„ì°©ì‹œê°„"] = f"ì¶œë°œ {round(time_before_departure.mean(), 1)}ë¶„ ì „"
                stats["ì¤‘ì•™ê°’_ë„ì°©ì‹œê°„"] = f"ì¶œë°œ {round(time_before_departure.median(), 1)}ë¶„ ì „"
                stats["ìµœì†Œ_ë„ì°©ì‹œê°„"] = f"ì¶œë°œ {round(time_before_departure.min(), 1)}ë¶„ ì „"
                stats["ìµœëŒ€_ë„ì°©ì‹œê°„"] = f"ì¶œë°œ {round(time_before_departure.max(), 1)}ë¶„ ì „"
                stats["í‘œì¤€í¸ì°¨_ë¶„"] = round(float(time_before_departure.std()), 1)

                # ì‹œê°„ëŒ€ë³„ ë¶„í¬
                if time_before_departure.mean() >= 60:
                    avg_hours = time_before_departure.mean() / 60
                    stats["ì„¤ëª…"] = f"ìŠ¹ê°ë“¤ì€ í‰ê· ì ìœ¼ë¡œ ì¶œë°œ {round(avg_hours, 1)}ì‹œê°„ ì „ì— ê³µí•­ì— ë„ì°©í•©ë‹ˆë‹¤"
                else:
                    stats["ì„¤ëª…"] = f"ìŠ¹ê°ë“¤ì€ í‰ê· ì ìœ¼ë¡œ ì¶œë°œ {round(time_before_departure.mean(), 1)}ë¶„ ì „ì— ê³µí•­ì— ë„ì°©í•©ë‹ˆë‹¤"

        return stats if stats else {"ì„¤ëª…": "ìŠ¹ê° ë„ì°© ì‹œê°„ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤"}

    def _analyze_destinations(self, df) -> Dict[str, Any]:
        """
        ëª©ì ì§€ë³„ í•­ê³µí¸ ë¶„ì„ (show-up-passenger.parquet ì „ìš©)

        Args:
            df: pandas DataFrame

        Returns:
            ëª©ì ì§€ë³„ í•­ê³µí¸ í†µê³„
        """
        import pandas as pd

        # arrival_city ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        if 'arrival_city' not in df.columns:
            return {
                "ì—ëŸ¬": "arrival_city ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤",
                "ì„¤ëª…": "ëª©ì ì§€ ì •ë³´ë¥¼ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤"
            }

        # ëª©ì ì§€ë³„ ë¶„ì„
        destinations = {}

        # ëª©ì ì§€ë³„ë¡œ ê·¸ë£¹í•‘ (NaN ì œì™¸)
        valid_destinations = df[df['arrival_city'].notna()]['arrival_city'].unique()

        for destination in sorted(valid_destinations):
            dest_df = df[df['arrival_city'] == destination]

            # í•­ê³µí¸ë³„ë¡œ ê·¸ë£¹í•‘ (operating_carrier_name + scheduled_departure_local)
            flight_groups = dest_df.groupby(['operating_carrier_name', 'scheduled_departure_local'])

            flights_list = []
            for (carrier, departure_time), flight_df in flight_groups:
                passenger_count = len(flight_df)

                # ì¶œë°œ ì‹œê° í¬ë§·íŒ… (HH:MM í˜•ì‹)
                if pd.notna(departure_time):
                    if isinstance(departure_time, str):
                        time_str = departure_time
                    else:
                        time_str = str(departure_time)

                    # ì‹œê°„ë§Œ ì¶”ì¶œ
                    try:
                        if 'T' in time_str:
                            time_part = time_str.split('T')[1].split('+')[0].split('.')[0][:5]
                        elif ' ' in time_str:
                            time_part = time_str.split(' ')[1][:5]
                        else:
                            time_part = time_str[:5]
                    except:
                        time_part = str(departure_time)[:16]
                else:
                    time_part = "N/A"

                # ì¢Œì„ ìˆ˜ ì •ë³´
                total_seats = flight_df['total_seats'].iloc[0] if 'total_seats' in flight_df.columns else None

                flight_info = {
                    "í•­ê³µì‚¬": str(carrier) if pd.notna(carrier) else "ì•Œ ìˆ˜ ì—†ìŒ",
                    "ì¶œë°œì‹œê°": time_part,
                    "ìŠ¹ê°ìˆ˜": passenger_count,
                }

                if total_seats is not None and pd.notna(total_seats):
                    flight_info["ì¢Œì„ìˆ˜"] = int(total_seats)
                    flight_info["íƒ‘ìŠ¹ë¥ _%"] = round(passenger_count / total_seats * 100, 1)

                flights_list.append(flight_info)

            # ì¶œë°œ ì‹œê° ìˆœìœ¼ë¡œ ì •ë ¬
            flights_list.sort(key=lambda x: x.get("ì¶œë°œì‹œê°", ""))

            destinations[str(destination)] = {
                "í•­ê³µí¸_ìˆ˜": len(flights_list),
                "ì´_ìŠ¹ê°_ìˆ˜": len(dest_df),
                "í•­ê³µí¸_ëª©ë¡": flights_list
            }

        # ì „ì²´ í†µê³„ ì¶”ê°€
        result = {
            "ì´_ëª©ì ì§€_ìˆ˜": len(destinations),
            "ëª©ì ì§€ë³„_ìƒì„¸": destinations
        }

        return result

    def _analyze_flights_in_simulation(self, df, process_names: list) -> Dict[str, Any]:
        """
        í•­ê³µí¸ë³„ ë¶„ì„ - ëª©ì ì§€, í•­ê³µí¸, ìŠ¹ê° ìˆ˜, ëŒ€ê¸° ì‹œê°„ í†µê³„

        show-up-passenger.parquetì˜ í•­ê³µí¸ ì •ë³´ë¥¼ í™œìš©í•˜ì—¬ ëª©ì ì§€ë³„/í•­ê³µí¸ë³„ í†µê³„ ìƒì„±
        Lambda ì‹œë®¬ë ˆì´ì…˜(lambda_function.py)ì€ show-up-passengerì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ìœ ì§€í•˜ê³ 
        ê° í”„ë¡œì„¸ìŠ¤ë³„ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì»¬ëŸ¼ë§Œ ì¶”ê°€í•¨

        Args:
            df: pandas DataFrame (simulation-pax.parquet)
            process_names: í”„ë¡œì„¸ìŠ¤ ì´ë¦„ ëª©ë¡

        Returns:
            í•­ê³µí¸ë³„ ë¶„ì„ ê²°ê³¼
        """
        import pandas as pd

        logger.info(f"Starting flight analysis. DataFrame shape: {df.shape}")
        logger.info(f"Available columns (first 30): {df.columns.tolist()[:30]}")

        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ (flight_numberëŠ” ì„ íƒ ì‚¬í•­)
        required_cols = ['arrival_city', 'scheduled_departure_local']
        missing_cols = [col for col in required_cols if col not in df.columns]

        # operating_carrier_iataë§Œ ì‚¬ìš© (ì‹¤ì œ ìš´í•­ í•­ê³µì‚¬)
        if 'operating_carrier_iata' not in df.columns:
            missing_cols.append('operating_carrier_iata')

        if missing_cols:
            logger.error(f"Missing flight columns: {missing_cols}. Cannot perform flight analysis.")
            logger.error(f"Available columns: {df.columns.tolist()}")
            return {
                "ì—ëŸ¬": "í•­ê³µí¸ ì •ë³´ ì»¬ëŸ¼ ëˆ„ë½",
                "ëˆ„ë½ëœ_ì»¬ëŸ¼": missing_cols,
                "ì‚¬ìš©ê°€ëŠ¥í•œ_ì»¬ëŸ¼": df.columns.tolist()[:30],  # ì²˜ìŒ 30ê°œë§Œ
                "ì„¤ëª…": "show-up-passenger.parquetì—ì„œ í•­ê³µí¸ ì •ë³´ê°€ ì œëŒ€ë¡œ ì „ë‹¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            }

        carrier_col = 'operating_carrier_iata'
        has_flight_number = 'flight_number' in df.columns and df['flight_number'].notna().any()
        logger.info(f"Using carrier column: {carrier_col}, has_flight_number: {has_flight_number}")

        flight_analysis = {
            "ëª©ì ì§€ë³„_ë¶„ì„": {},
            "ì „ì²´_í•­ê³µí¸_í†µê³„": {
                "ì´_í•­ê³µí¸_ìˆ˜": 0,
                "ì´_ëª©ì ì§€_ìˆ˜": 0
            }
        }

        # ëª©ì ì§€ë³„ë¡œ ê·¸ë£¹í•‘ (NaN ì œì™¸)
        destinations = df['arrival_city'].dropna().unique()
        valid_dest_count = len(destinations)

        logger.info(f"Found {valid_dest_count} valid destinations (excluding NaN)")

        if valid_dest_count == 0:
            logger.warning("No valid destinations found in arrival_city column")
            return {
                "ëª©ì ì§€ë³„_ë¶„ì„": {},
                "ì „ì²´_í•­ê³µí¸_í†µê³„": {
                    "ì´_í•­ê³µí¸_ìˆ˜": 0,
                    "ì´_ëª©ì ì§€_ìˆ˜": 0
                },
                "ê²½ê³ ": "arrival_city ì»¬ëŸ¼ì— ìœ íš¨í•œ ëª©ì ì§€ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"
            }

        flight_analysis["ì „ì²´_í•­ê³µí¸_í†µê³„"]["ì´_ëª©ì ì§€_ìˆ˜"] = valid_dest_count

        total_flights = 0

        for destination in sorted(destinations):
            if pd.isna(destination):
                continue

            dest_df = df[df['arrival_city'] == destination]

            # í•­ê³µí¸ë³„ë¡œ ê·¸ë£¹í•‘ (carrier_col + scheduled_departure_local, flight_numberëŠ” ì„ íƒ)
            if has_flight_number:
                flight_groups = dest_df.groupby([carrier_col, 'flight_number', 'scheduled_departure_local'])
            else:
                flight_groups = dest_df.groupby([carrier_col, 'scheduled_departure_local'])

            flights_list = []

            for group_keys, flight_df in flight_groups:
                # flight_number ìœ ë¬´ì— ë”°ë¼ unpacking
                if has_flight_number:
                    carrier, flight_num, departure_time = group_keys
                else:
                    carrier, departure_time = group_keys
                    flight_num = None
                passenger_count = len(flight_df)

                # ì¶œë°œ ì‹œê° í¬ë§·íŒ…
                if pd.notna(departure_time):
                    if isinstance(departure_time, str):
                        departure_time_str = departure_time
                    else:
                        departure_time_str = str(departure_time)

                    # ì‹œê°„ë§Œ ì¶”ì¶œ (HH:MM í˜•ì‹)
                    try:
                        if 'T' in departure_time_str:
                            time_part = departure_time_str.split('T')[1].split('+')[0].split('.')[0][:5]
                        else:
                            time_part = departure_time_str.split(' ')[1][:5] if ' ' in departure_time_str else departure_time_str[:5]
                    except:
                        time_part = str(departure_time)[:16]
                else:
                    time_part = "N/A"

                # carrier ê°’ ì •ë¦¬ (IATA ì½”ë“œ)
                carrier_code = str(carrier) if pd.notna(carrier) else ""

                flight_info = {
                    "í•­ê³µì‚¬_ì½”ë“œ": carrier_code,
                    "í•­ê³µì‚¬": carrier_code,  # í˜¸í™˜ì„± ìœ ì§€
                    "ì¶œë°œì‹œê°": time_part,
                    "ìŠ¹ê°ìˆ˜": passenger_count
                }

                # flight_numberê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì¶”ê°€
                if flight_num is not None and pd.notna(flight_num):
                    flight_info["í¸ëª…"] = f"{carrier_code}{flight_num}" if carrier_code else str(flight_num)

                # ê° í”„ë¡œì„¸ìŠ¤ë³„ í‰ê·  ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                for process_name in process_names:
                    status_col = f"{process_name}_status"
                    queue_wait_col = f"{process_name}_queue_wait_time"

                    # completed ìŠ¹ê°ë§Œ ëŒ€ìƒ
                    if status_col in flight_df.columns:
                        completed_mask = flight_df[status_col] == "completed"

                        if queue_wait_col in flight_df.columns:
                            queue_wait_times = flight_df.loc[completed_mask, queue_wait_col].dropna()

                            if len(queue_wait_times) > 0 and pd.api.types.is_timedelta64_dtype(queue_wait_times):
                                avg_wait_seconds = queue_wait_times.dt.total_seconds().mean()
                                avg_wait_minutes = avg_wait_seconds / 60
                                flight_info[f"{process_name}_í‰ê· ëŒ€ê¸°_ë¶„"] = round(float(avg_wait_minutes), 2)
                            else:
                                flight_info[f"{process_name}_í‰ê· ëŒ€ê¸°_ë¶„"] = 0.0
                        else:
                            flight_info[f"{process_name}_í‰ê· ëŒ€ê¸°_ë¶„"] = 0.0

                flights_list.append(flight_info)

            # ëª©ì ì§€ë³„ í†µê³„ ìƒì„±
            flight_analysis["ëª©ì ì§€ë³„_ë¶„ì„"][str(destination)] = {
                "í•­ê³µí¸_ìˆ˜": len(flights_list),
                "ì´_ìŠ¹ê°_ìˆ˜": len(dest_df),
                "í•­ê³µí¸_ëª©ë¡": sorted(flights_list, key=lambda x: x.get("ì¶œë°œì‹œê°", ""))
            }

            total_flights += len(flights_list)

        flight_analysis["ì „ì²´_í•­ê³µí¸_í†µê³„"]["ì´_í•­ê³µí¸_ìˆ˜"] = total_flights

        return flight_analysis

    async def _analyze_simulation_pax_with_response(self, df, scenario_id: str) -> Dict[str, Any]:
        """
        simulation-pax.parquet ë¶„ì„ - Lambda ì‹œë®¬ë ˆì´ì…˜ ë¡œì§ ê¸°ë°˜ ì„¤ëª…

        Lambda í•¨ìˆ˜(lambda_function.py)ê°€ ìˆ˜í–‰í•˜ëŠ” ì´ë²¤íŠ¸ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ì˜ ê²°ê³¼ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤:

        **ì¤‘ìš”: LambdaëŠ” show-up-passenger.parquetì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ìœ ì§€í•©ë‹ˆë‹¤**
        - lambda_function.pyì˜ process_all_steps()ì—ì„œ df.copy()ë¡œ ì›ë³¸ ìœ ì§€
        - ì¦‰, arrival_city, carrier, flight_number, scheduled_departure_local ë“± ëª¨ë“  í•­ê³µí¸ ì •ë³´ í¬í•¨
        - ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ ì»¬ëŸ¼ë§Œ ì¶”ê°€: {process}_status, {process}_queue_wait_time ë“±

        ì‹œë®¬ë ˆì´ì…˜ ì›ë¦¬:
        1. ê° í”„ë¡œì„¸ìŠ¤ë³„ë¡œ ìŠ¹ê°ì´ show_up_time ë˜ëŠ” ì´ì „ ë‹¨ê³„ done_timeì—ì„œ ì‹œì‘
        2. travel_time_minutesë¥¼ ë”í•´ í”„ë¡œì„¸ìŠ¤ ë„ì°© ì˜ˆì • ì‹œê°„(on_pred) ê³„ì‚°
        3. ê°€ëŠ¥í•œ ì‹œì„¤ ì¤‘ ê°€ì¥ ë¹¨ë¦¬ ì²˜ë¦¬ë  ì‹œì„¤ì— ë°°ì •
        4. open_wait_time(ì‹œì„¤ ì˜¤í”ˆ ëŒ€ê¸°) + queue_wait_time(í ëŒ€ê¸°) ê³„ì‚°
        5. status: completed(ì²˜ë¦¬ ì™„ë£Œ), failed(ì‹œì„¤ ì—†ìŒ), skipped(ì¡°ê±´ ë¯¸ì¶©ì¡±)

        Args:
            df: pandas DataFrame (simulation-pax.parquet)
            scenario_id: ì‹œë‚˜ë¦¬ì˜¤ ID

        Returns:
            Lambda ì‹œë®¬ë ˆì´ì…˜ ì›ë¦¬ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ + í•­ê³µí¸ë³„ ë¶„ì„
        """
        import pandas as pd
        import numpy as np

        logger.info(f"Analyzing simulation-pax.parquet: {len(df)} rows, {len(df.columns)} columns")
        logger.info(f"Checking for flight info columns: arrival_city={('arrival_city' in df.columns)}, carrier={('carrier' in df.columns)}, flight_number={('flight_number' in df.columns)}")

        # metadata.jsonì—ì„œ process_flow ì •ë³´ ë¡œë“œ
        metadata = None
        try:
            metadata = await self.simulation_service.s3_manager.get_json_async(
                scenario_id=scenario_id,
                filename="metadata.json"
            )
        except Exception as e:
            logger.warning(f"Could not load metadata.json: {str(e)}")

        # í”„ë¡œì„¸ìŠ¤ ì»¬ëŸ¼ ìë™ ê°ì§€
        process_names = set()
        for col in df.columns:
            if "_status" in col:
                process_name = col.replace("_status", "")
                process_names.add(process_name)

        process_names = sorted(process_names)

        # ê¸°ë³¸ ì •ë³´
        analysis = {
            "íŒŒì¼ëª…": "simulation-pax.parquet",
            "ê¸°ë³¸_ì •ë³´": {
                "ì´_ìŠ¹ê°_ìˆ˜": len(df),
                "ì´_í”„ë¡œì„¸ìŠ¤_ìˆ˜": len(process_names),
                "í”„ë¡œì„¸ìŠ¤_ëª©ë¡": process_names,
            },
            "ì‹œë®¬ë ˆì´ì…˜_ì›ë¦¬": {
                "ì„¤ëª…": "ì´ë²¤íŠ¸ ê¸°ë°˜ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ ê° ìŠ¹ê°ì´ í”„ë¡œì„¸ìŠ¤ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ê±°ì¹˜ë©° ì²˜ë¦¬ë©ë‹ˆë‹¤.",
                "ì²˜ë¦¬_íë¦„": [
                    "1. ìŠ¹ê° ê³µí•­ ë„ì°© (show_up_time)",
                    "2. ê° í”„ë¡œì„¸ìŠ¤ë¡œ ì´ë™ (travel_time_minutes ì†Œìš”)",
                    "3. ê°€ëŠ¥í•œ ì‹œì„¤ ì¤‘ ê°€ì¥ ë¹¨ë¦¬ ì²˜ë¦¬ë  ì‹œì„¤ ë°°ì •",
                    "4. ì‹œì„¤ ì˜¤í”ˆ ëŒ€ê¸° (open_wait_time) + í ëŒ€ê¸° (queue_wait_time)",
                    "5. ì„œë¹„ìŠ¤ ì²˜ë¦¬ í›„ ë‹¤ìŒ í”„ë¡œì„¸ìŠ¤ë¡œ ì´ë™"
                ],
                "status_ì˜ë¯¸": {
                    "completed": "ì •ìƒ ì²˜ë¦¬ ì™„ë£Œ",
                    "failed": "ì‹œì„¤ ë°°ì • ì‹¤íŒ¨ (ìš´ì˜ ì‹œê°„ ë¶€ì¡± ë˜ëŠ” ì¡°ê±´ ë¶ˆì¼ì¹˜)",
                    "skipped": "ì¡°ê±´ ë¯¸ì¶©ì¡±ìœ¼ë¡œ ê±´ë„ˆëœ€ (entry_conditions)"
                }
            },
            "í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„": {},
            "ì „ì²´_ì²˜ë¦¬_í†µê³„": {}
        }

        # metadataì—ì„œ ê° í”„ë¡œì„¸ìŠ¤ ì„¤ì • ì •ë³´ ì¶”ì¶œ
        process_configs = {}
        if metadata and "process_flow" in metadata:
            for proc in metadata["process_flow"]:
                proc_name = proc.get("name")
                process_configs[proc_name] = {
                    "travel_time_minutes": proc.get("travel_time_minutes", 0),
                    "ì´_zone_ìˆ˜": len(proc.get("zones", {})),
                    "ì´_ì‹œì„¤_ìˆ˜": sum(
                        len(zone.get("facilities", []))
                        for zone in proc.get("zones", {}).values()
                    ),
                    "entry_conditions": proc.get("entry_conditions", [])
                }

        # ê° í”„ë¡œì„¸ìŠ¤ë³„ ë¶„ì„
        for process_name in process_names:
            process_analysis = {
                "í”„ë¡œì„¸ìŠ¤_ì„¤ì •": process_configs.get(process_name, {}),
            }

            # Status ë¶„ì„
            status_col = f"{process_name}_status"
            if status_col in df.columns:
                status_counts = df[status_col].value_counts().to_dict()
                total = len(df)

                process_analysis["ì²˜ë¦¬_ê²°ê³¼"] = {
                    "completed": {
                        "ìŠ¹ê°_ìˆ˜": status_counts.get("completed", 0),
                        "ë¹„ìœ¨_%": round(status_counts.get("completed", 0) / total * 100, 2)
                    },
                    "failed": {
                        "ìŠ¹ê°_ìˆ˜": status_counts.get("failed", 0),
                        "ë¹„ìœ¨_%": round(status_counts.get("failed", 0) / total * 100, 2)
                    },
                    "skipped": {
                        "ìŠ¹ê°_ìˆ˜": status_counts.get("skipped", 0),
                        "ë¹„ìœ¨_%": round(status_counts.get("skipped", 0) / total * 100, 2)
                    }
                }

            # ëŒ€ê¸° ì‹œê°„ ë¶„ì„ (completed ìŠ¹ê°ë§Œ)
            completed_mask = df[status_col] == "completed" if status_col in df.columns else pd.Series(False, index=df.index)

            # open_wait_time: ì‹œì„¤ ì˜¤í”ˆ ëŒ€ê¸° ì‹œê°„
            open_wait_col = f"{process_name}_open_wait_time"
            if open_wait_col in df.columns:
                open_wait_times = df.loc[completed_mask, open_wait_col].dropna()
                if len(open_wait_times) > 0 and pd.api.types.is_timedelta64_dtype(open_wait_times):
                    open_wait_seconds = open_wait_times.dt.total_seconds()
                    open_wait_minutes = open_wait_seconds / 60

                    process_analysis["ì˜¤í”ˆ_ëŒ€ê¸°ì‹œê°„_ë¶„"] = {
                        "í‰ê· ": round(float(open_wait_minutes.mean()), 2),
                        "ì¤‘ì•™ê°’": round(float(open_wait_minutes.median()), 2),
                        "ìµœëŒ€": round(float(open_wait_minutes.max()), 2),
                        "ì„¤ëª…": "ì‹œì„¤ì´ ì•„ì§ ì˜¤í”ˆí•˜ì§€ ì•Šì•„ì„œ ê¸°ë‹¤ë¦° ì‹œê°„"
                    }

            # queue_wait_time: í ëŒ€ê¸° ì‹œê°„
            queue_wait_col = f"{process_name}_queue_wait_time"
            if queue_wait_col in df.columns:
                queue_wait_times = df.loc[completed_mask, queue_wait_col].dropna()
                if len(queue_wait_times) > 0 and pd.api.types.is_timedelta64_dtype(queue_wait_times):
                    queue_wait_seconds = queue_wait_times.dt.total_seconds()
                    queue_wait_minutes = queue_wait_seconds / 60

                    process_analysis["í_ëŒ€ê¸°ì‹œê°„_ë¶„"] = {
                        "í‰ê· ": round(float(queue_wait_minutes.mean()), 2),
                        "ì¤‘ì•™ê°’": round(float(queue_wait_minutes.median()), 2),
                        "ìµœëŒ€": round(float(queue_wait_minutes.max()), 2),
                        "ì„¤ëª…": "ì‹œì„¤ì´ ì´ë¯¸ ë‹¤ë¥¸ ìŠ¹ê°ì„ ì²˜ë¦¬ ì¤‘ì´ì–´ì„œ ëŒ€ê¸°í•œ ì‹œê°„"
                    }

            # í ê¸¸ì´ ë¶„ì„
            queue_length_col = f"{process_name}_queue_length"
            if queue_length_col in df.columns:
                queue_lengths = df.loc[completed_mask, queue_length_col].dropna()
                if len(queue_lengths) > 0:
                    process_analysis["í_ê¸¸ì´_í†µê³„"] = {
                        "í‰ê· ": round(float(queue_lengths.mean()), 2),
                        "ì¤‘ì•™ê°’": round(float(queue_lengths.median()), 2),
                        "ìµœëŒ€": int(queue_lengths.max()),
                        "ì„¤ëª…": "ìŠ¹ê°ì´ ë„ì°©í–ˆì„ ë•Œ ì•ì— ëŒ€ê¸° ì¤‘ì¸ ìŠ¹ê° ìˆ˜"
                    }

            # ì‹œì„¤ ì‚¬ìš© ë¶„ì„
            facility_col = f"{process_name}_facility"
            if facility_col in df.columns:
                facility_counts = df.loc[completed_mask, facility_col].value_counts()

                process_analysis["ì‹œì„¤_í™œìš©ë„"] = {
                    "ì´_ì‚¬ìš©ëœ_ì‹œì„¤_ìˆ˜": len(facility_counts),
                    "ìƒìœ„_10ê°œ_ì‹œì„¤": facility_counts.head(10).to_dict(),
                    "ì„¤ëª…": "ê° ì‹œì„¤ì´ ì²˜ë¦¬í•œ ìŠ¹ê° ìˆ˜"
                }

            # Zone ë¶„í¬
            zone_col = f"{process_name}_zone"
            if zone_col in df.columns:
                zone_counts = df.loc[completed_mask, zone_col].value_counts().to_dict()
                process_analysis["zone_ë¶„í¬"] = {
                    str(k): int(v) for k, v in zone_counts.items()
                }

            analysis["í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„"][process_name] = process_analysis

        # ì „ì²´ ì²˜ë¦¬ í†µê³„
        if process_names:
            first_process = process_names[0]
            last_process = process_names[-1]

            # ì „ì²´ ì™„ë£Œìœ¨ (ë§ˆì§€ë§‰ í”„ë¡œì„¸ìŠ¤ ê¸°ì¤€)
            last_status_col = f"{last_process}_status"
            if last_status_col in df.columns:
                last_status_counts = df[last_status_col].value_counts().to_dict()
                total = len(df)

                analysis["ì „ì²´_ì²˜ë¦¬_í†µê³„"]["ìµœì¢…_ì™„ë£Œìœ¨_%"] = round(
                    last_status_counts.get("completed", 0) / total * 100, 2
                )

            # ì´ ì²˜ë¦¬ ì‹œê°„ (ì²« í”„ë¡œì„¸ìŠ¤ ì‹œì‘ ~ ë§ˆì§€ë§‰ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ)
            first_start_col = f"{first_process}_start_time"
            last_done_col = f"{last_process}_done_time"

            if first_start_col in df.columns and last_done_col in df.columns:
                df_temp = df.copy()
                df_temp[first_start_col] = pd.to_datetime(df_temp[first_start_col])
                df_temp[last_done_col] = pd.to_datetime(df_temp[last_done_col])

                # ì™„ë£Œëœ ìŠ¹ê°ë§Œ
                completed_mask = (df_temp[first_start_col].notna()) & (df_temp[last_done_col].notna())
                if completed_mask.sum() > 0:
                    total_times = (df_temp.loc[completed_mask, last_done_col] -
                                   df_temp.loc[completed_mask, first_start_col]).dt.total_seconds() / 60

                    analysis["ì „ì²´_ì²˜ë¦¬_í†µê³„"]["ì´_ì²˜ë¦¬ì‹œê°„_ë¶„"] = {
                        "í‰ê· ": round(float(total_times.mean()), 2),
                        "ì¤‘ì•™ê°’": round(float(total_times.median()), 2),
                        "ìµœì†Œ": round(float(total_times.min()), 2),
                        "ìµœëŒ€": round(float(total_times.max()), 2),
                        "ì„¤ëª…": "ì²« í”„ë¡œì„¸ìŠ¤ ì‹œì‘ë¶€í„° ë§ˆì§€ë§‰ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œê¹Œì§€ ì†Œìš” ì‹œê°„"
                    }

            # ë³‘ëª© í”„ë¡œì„¸ìŠ¤ ì°¾ê¸° (í‰ê·  í ëŒ€ê¸°ì‹œê°„ì´ ê°€ì¥ ê¸´ í”„ë¡œì„¸ìŠ¤)
            max_queue_wait = 0
            bottleneck_process = None

            for process_name in process_names:
                if process_name in analysis["í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„"]:
                    queue_wait = analysis["í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„"][process_name].get("í_ëŒ€ê¸°ì‹œê°„_ë¶„", {})
                    avg_wait = queue_wait.get("í‰ê· ", 0)
                    if avg_wait > max_queue_wait:
                        max_queue_wait = avg_wait
                        bottleneck_process = process_name

            if bottleneck_process:
                analysis["ì „ì²´_ì²˜ë¦¬_í†µê³„"]["ë³‘ëª©_í”„ë¡œì„¸ìŠ¤"] = {
                    "í”„ë¡œì„¸ìŠ¤_ì´ë¦„": bottleneck_process,
                    "í‰ê· _í_ëŒ€ê¸°ì‹œê°„_ë¶„": round(max_queue_wait, 2),
                    "ì„¤ëª…": "í ëŒ€ê¸°ì‹œê°„ì´ ê°€ì¥ ê¸´ í”„ë¡œì„¸ìŠ¤"
                }

        # í•­ê³µí¸ë³„ ë¶„ì„ ì¶”ê°€ (LambdaëŠ” show-up-passengerì˜ ëª¨ë“  ì»¬ëŸ¼ì„ ìœ ì§€í•¨)
        logger.info("Calling _analyze_flights_in_simulation...")
        flight_analysis = self._analyze_flights_in_simulation(df, process_names)

        if flight_analysis is None:
            logger.error("Flight analysis returned None - this should not happen")
            flight_analysis = {
                "ì—ëŸ¬": "í•­ê³µí¸ ë¶„ì„ ì‹¤íŒ¨",
                "ì„¤ëª…": "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ë¡œ í•­ê³µí¸ ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            }

        analysis["í•­ê³µí¸ë³„_ë¶„ì„"] = flight_analysis
        logger.info(f"Flight analysis completed. Keys: {list(flight_analysis.keys())}")

        # summary_info ìƒì„± (AIì—ê²Œ ì „ë‹¬í•  í˜•ì‹)
        # í•­ê³µí¸ë³„_ë¶„ì„ì„ ì•ìª½ì— ë°°ì¹˜í•˜ì—¬ JSON ì§ë ¬í™” ì‹œ 20000ì ì œí•œì— ì˜ë¦¬ì§€ ì•Šë„ë¡ í•¨
        summary_info = {
            "íŒŒì¼ëª…": "simulation-pax.parquet",
            "ê¸°ë³¸_ì •ë³´": analysis["ê¸°ë³¸_ì •ë³´"],
            "í•­ê³µí¸ë³„_ë¶„ì„": analysis["í•­ê³µí¸ë³„_ë¶„ì„"],  # ì‚¬ìš©ì ì§ˆë¬¸ì— ì¤‘ìš”í•œ ì •ë³´ì´ë¯€ë¡œ ì•ìª½ ë°°ì¹˜
            "ì‹œë®¬ë ˆì´ì…˜_ì›ë¦¬": analysis["ì‹œë®¬ë ˆì´ì…˜_ì›ë¦¬"],
            "ì „ì²´_ì²˜ë¦¬_í†µê³„": analysis["ì „ì²´_ì²˜ë¦¬_í†µê³„"],
            "í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„": analysis["í”„ë¡œì„¸ìŠ¤ë³„_ë¶„ì„"],  # ìƒì„¸í•œ ì •ë³´ëŠ” ë’¤ìª½ ë°°ì¹˜
        }

        structure_str = f"ì´ {len(df):,}ëª…, {len(df.columns)}ê°œ ì»¬ëŸ¼, {len(process_names)}ê°œ í”„ë¡œì„¸ìŠ¤"

        full_summary = f"íŒŒì¼: simulation-pax.parquet\n"
        full_summary += f"ì´ ìŠ¹ê° ìˆ˜: {len(df):,}ëª…\n"
        full_summary += f"í”„ë¡œì„¸ìŠ¤ ìˆ˜: {len(process_names)}ê°œ\n"
        full_summary += f"í”„ë¡œì„¸ìŠ¤ ëª©ë¡: {', '.join(process_names)}\n"

        # í•­ê³µí¸ë³„ ë¶„ì„ ìš”ì•½
        if "ì „ì²´_í•­ê³µí¸_í†µê³„" in flight_analysis:
            stats = flight_analysis["ì „ì²´_í•­ê³µí¸_í†µê³„"]
            full_summary += f"ì´ ëª©ì ì§€ ìˆ˜: {stats.get('ì´_ëª©ì ì§€_ìˆ˜', 0)}ê°œ\n"
            full_summary += f"ì´ í•­ê³µí¸ ìˆ˜: {stats.get('ì´_í•­ê³µí¸_ìˆ˜', 0)}í¸\n"
        elif "ì—ëŸ¬" in flight_analysis:
            full_summary += f"í•­ê³µí¸ ë¶„ì„ ì—ëŸ¬: {flight_analysis.get('ì„¤ëª…', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')}\n"

        if "ìµœì¢…_ì™„ë£Œìœ¨_%" in analysis["ì „ì²´_ì²˜ë¦¬_í†µê³„"]:
            full_summary += f"ìµœì¢… ì™„ë£Œìœ¨: {analysis['ì „ì²´_ì²˜ë¦¬_í†µê³„']['ìµœì¢…_ì™„ë£Œìœ¨_%']}%\n"

        return {
            "summary_info": summary_info,
            "structure": {"rows": len(df), "columns": df.columns.tolist()},
            "structure_str": structure_str,
            "full_summary": full_summary,
        }

    async def get_scenario_context(self, scenario_id: str) -> Dict[str, Any]:
        """
        ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì¡°íšŒ (AIì—ê²Œ ì œê³µí•  ì •ë³´)

        Returns:
            ì‹œë‚˜ë¦¬ì˜¤ ì»¨í…ìŠ¤íŠ¸ ì •ë³´
        """
        try:
            metadata_result = await self.simulation_service.load_scenario_metadata(scenario_id)
            metadata = metadata_result.get("metadata", {})

            if metadata is None:
                return {
                    "scenario_id": scenario_id,
                    "process_flow": [],
                    "has_metadata": False,
                }

            process_flow = metadata.get("process_flow", [])

            return {
                "scenario_id": scenario_id,
                "process_flow": process_flow,
                "process_count": len(process_flow),
                "process_names": [p.get("name") for p in process_flow],
                "has_metadata": True,
                "context": metadata.get("context", {}),
            }

        except Exception as e:
            logger.error(f"Failed to get scenario context: {str(e)}")
            return {
                "scenario_id": scenario_id,
                "error": str(e),
            }
